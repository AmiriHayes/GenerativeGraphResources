{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDSSL Node Classification - Worker\n",
    "\n",
    "**To Do**:\n",
    "- Wy is the GCN/graphSAGE approach so fast/why is the MLP approach so slow?\n",
    "- The MPNN approaches beat the MLP approach, but neither are as good as I expected\n",
    "- I tried using class weights, and results aren't great\n",
    "- does it learn that everyone in a household has the same income?\n",
    "- add more descriptive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.data import GraphSAINTRandomWalkSampler\n",
    "from torch_geometric.utils import get_laplacian, degree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.style as style \n",
    "style.use('seaborn-paper')\n",
    "\n",
    "fontsize = 12\n",
    "plt.rcParams.update({\n",
    "    'font.size': fontsize, \n",
    "    'axes.labelsize': fontsize, \n",
    "    'legend.fontsize': fontsize,\n",
    "    'xtick.labelsize': fontsize,\n",
    "    'ytick.labelsize': fontsize,\n",
    "    'axes.titlesize': fontsize\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#from imports import *\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Convert the node attribute data into an (x,y) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = pd.read_csv('../../data/NDSSL data/raw/node_attributes.csv')\n",
    "\n",
    "## one-hot encode gender\n",
    "gender_index = torch.LongTensor(node_attributes['gender'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "gender_onehot = torch.LongTensor(len(node_attributes), 2)\n",
    "gender_onehot.zero_()\n",
    "gender_onehot = gender_onehot.scatter_(1, gender_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode worker\n",
    "worker_index = torch.LongTensor(node_attributes['worker'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "worker_onehot = torch.LongTensor(len(node_attributes), 2)\n",
    "worker_onehot.zero_()\n",
    "worker_onehot = worker_onehot.scatter_(1, worker_index, 1).type(torch.float32);\n",
    "\n",
    "## map the 117 distinct zipcodes to the integers 0, ..., 116\n",
    "zipcode_original = node_attributes['zipcode'].values\n",
    "zipcode_dict = {i: j for j, i in enumerate(set(zipcode_original))} \n",
    "zipcode_index = torch.LongTensor(np.asarray([zipcode_dict[i] for i in zipcode_original])).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "\n",
    "## one-hot encode zipcode\n",
    "zipcode_onehot = torch.LongTensor(len(node_attributes), len(zipcode_dict))\n",
    "zipcode_onehot.zero_()\n",
    "zipcode_onehot = zipcode_onehot.scatter_(1, zipcode_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode household income\n",
    "household_income_index = torch.LongTensor(node_attributes['household_income'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "household_income_onehot = torch.LongTensor(len(node_attributes), 14)\n",
    "household_income_onehot.zero_()\n",
    "household_income_onehot = household_income_onehot.scatter_(1, household_income_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode relationship\n",
    "relationship_index = torch.LongTensor(node_attributes['relationship'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "relationship_onehot = torch.LongTensor(len(node_attributes), 4)\n",
    "relationship_onehot.zero_()\n",
    "relationship_onehot = relationship_onehot.scatter_(1, relationship_index, 1).type(torch.float32);\n",
    "\n",
    "age = torch.FloatTensor(node_attributes['age'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_size = torch.FloatTensor(node_attributes['household_size'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_workers = torch.FloatTensor(node_attributes['household_workers'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_vehicles = torch.FloatTensor(node_attributes['household_vehicles'].values).reshape(len(node_attributes), 1).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1601330, 24]) torch.float32\n",
      "torch.Size([1601330]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.cat((gender_onehot, age, relationship_onehot, household_income_onehot, household_size, household_workers, household_vehicles), dim=1)\n",
    "#zipcode_onehot\n",
    "y = worker_index[:,0]\n",
    "\n",
    "#maybe remove household workers\n",
    "\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct households: 632626\n",
      "number of training households: 506100\n",
      "number of testing households: 126526\n",
      "size of training set: 1280901\n",
      "size of testing set: 320429\n",
      "percent train: 0.80\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "## create a train/test split by household\n",
    "household_ids = list(set(list(node_attributes['household_id'])))\n",
    "np.random.shuffle(household_ids)\n",
    "\n",
    "ntrain_households = int(0.8 * len(household_ids))\n",
    "households_train = household_ids[:ntrain_households]\n",
    "households_test = household_ids[ntrain_households:]\n",
    "print('number of distinct households: %i' %len(household_ids))\n",
    "print('number of training households: %i' %len(households_train))\n",
    "print('number of testing households: %i' %len(households_test))\n",
    "\n",
    "train_mask = torch.IntTensor(node_attributes['household_id'].isin(households_train)).type(torch.int64)\n",
    "test_mask = torch.IntTensor(node_attributes['household_id'].isin(households_test)).type(torch.int64)\n",
    "train_idx = np.arange(len(node_attributes))[train_mask == 1]\n",
    "test_idx = np.arange(len(node_attributes))[test_mask == 1]\n",
    "\n",
    "print('size of training set: %i' %torch.sum(train_mask).item())\n",
    "print('size of testing set: %i' %torch.sum(test_mask).item())\n",
    "print('percent train: %.2f' %(torch.sum(train_mask).item()/len(train_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, '../../data/NDSSL data/raw/x.pt')\n",
    "torch.save(y, '../../data/NDSSL data/raw/y.pt')\n",
    "torch.save(train_mask, '../../data/NDSSL data/raw/train_mask.pt')\n",
    "torch.save(test_mask, '../../data/NDSSL data/raw/test_mask.pt')\n",
    "torch.save(train_idx, '../../data/NDSSL data/raw/train_idx.pt')\n",
    "torch.save(test_idx, '../../data/NDSSL data/raw/test_idx.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification: MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = torch.utils.data.TensorDataset(x, y)\n",
    "train_set = torch.utils.data.Subset(full_set, train_idx)\n",
    "test_set = torch.utils.data.Subset(full_set, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(mlp, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)    \n",
    "        x = x.log_softmax(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        loss = F.nll_loss(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_batch.shape[0]\n",
    "        total_examples += x_batch.shape[0]\n",
    "    \n",
    "    return total_loss/total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    ## train accuracy\n",
    "    total_examples = correct = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        total_examples += out.shape[0]\n",
    "        correct += torch.sum(torch.argmax(out, axis=1) == y_batch).cpu().item()\n",
    "    train_accuracy = correct/total_examples\n",
    "    \n",
    "    ## test accuracy\n",
    "    total_examples = correct = 0.0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        total_examples += x_batch.shape[0]\n",
    "        correct += torch.sum(torch.argmax(out, axis=1) == y_batch).cpu().item()\n",
    "    test_accuracy = correct/total_examples\n",
    "    \n",
    "    return train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: why is this so slow, even with a GPU?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 1028002\n",
      "Epoch: 01, Loss: 0.6853, Train: 0.6829, Test: 0.6818\n",
      "Epoch: 02, Loss: 0.5834, Train: 0.6989, Test: 0.6976\n",
      "Epoch: 03, Loss: 0.5320, Train: 0.7953, Test: 0.7949\n",
      "Epoch: 04, Loss: 0.4983, Train: 0.8275, Test: 0.8269\n",
      "Epoch: 05, Loss: 0.4733, Train: 0.8322, Test: 0.8315\n",
      "Epoch: 06, Loss: 0.4534, Train: 0.8344, Test: 0.8340\n",
      "Epoch: 07, Loss: 0.4374, Train: 0.8379, Test: 0.8374\n",
      "Epoch: 08, Loss: 0.4214, Train: 0.8413, Test: 0.8409\n",
      "Epoch: 09, Loss: 0.4075, Train: 0.8442, Test: 0.8432\n",
      "Epoch: 10, Loss: 0.3943, Train: 0.8493, Test: 0.8485\n",
      "Epoch: 11, Loss: 0.3814, Train: 0.8569, Test: 0.8562\n",
      "Epoch: 12, Loss: 0.3686, Train: 0.8587, Test: 0.8578\n",
      "Epoch: 13, Loss: 0.3574, Train: 0.8629, Test: 0.8624\n",
      "Epoch: 14, Loss: 0.3475, Train: 0.8681, Test: 0.8676\n",
      "Epoch: 15, Loss: 0.3378, Train: 0.8745, Test: 0.8738\n",
      "Epoch: 16, Loss: 0.3285, Train: 0.8792, Test: 0.8789\n",
      "Epoch: 17, Loss: 0.3205, Train: 0.8831, Test: 0.8828\n",
      "Epoch: 18, Loss: 0.3130, Train: 0.8879, Test: 0.8877\n",
      "Epoch: 19, Loss: 0.3066, Train: 0.8927, Test: 0.8926\n",
      "Epoch: 20, Loss: 0.3001, Train: 0.8929, Test: 0.8926\n",
      "Epoch: 21, Loss: 0.2945, Train: 0.8977, Test: 0.8974\n",
      "Epoch: 22, Loss: 0.2892, Train: 0.8968, Test: 0.8965\n",
      "Epoch: 23, Loss: 0.2846, Train: 0.8996, Test: 0.8992\n",
      "Epoch: 24, Loss: 0.2805, Train: 0.9001, Test: 0.8997\n",
      "Epoch: 25, Loss: 0.2765, Train: 0.9033, Test: 0.9031\n",
      "Epoch: 26, Loss: 0.2729, Train: 0.9041, Test: 0.9035\n",
      "Epoch: 27, Loss: 0.2690, Train: 0.9034, Test: 0.9029\n",
      "Epoch: 28, Loss: 0.2655, Train: 0.9060, Test: 0.9053\n",
      "Epoch: 29, Loss: 0.2631, Train: 0.9085, Test: 0.9078\n",
      "Epoch: 30, Loss: 0.2601, Train: 0.9082, Test: 0.9077\n",
      "Epoch: 31, Loss: 0.2574, Train: 0.9088, Test: 0.9084\n",
      "Epoch: 32, Loss: 0.2547, Train: 0.9097, Test: 0.9092\n",
      "Epoch: 33, Loss: 0.2524, Train: 0.9118, Test: 0.9115\n",
      "Epoch: 34, Loss: 0.2502, Train: 0.9109, Test: 0.9104\n",
      "Epoch: 35, Loss: 0.2479, Train: 0.9124, Test: 0.9121\n",
      "Epoch: 36, Loss: 0.2460, Train: 0.9117, Test: 0.9113\n",
      "Epoch: 37, Loss: 0.2437, Train: 0.9138, Test: 0.9136\n",
      "Epoch: 38, Loss: 0.2421, Train: 0.9129, Test: 0.9124\n",
      "Epoch: 39, Loss: 0.2400, Train: 0.9155, Test: 0.9152\n",
      "Epoch: 40, Loss: 0.2380, Train: 0.9144, Test: 0.9139\n",
      "Epoch: 41, Loss: 0.2367, Train: 0.9172, Test: 0.9169\n",
      "Epoch: 42, Loss: 0.2351, Train: 0.9171, Test: 0.9168\n",
      "Epoch: 43, Loss: 0.2337, Train: 0.9181, Test: 0.9176\n",
      "Epoch: 44, Loss: 0.2323, Train: 0.9176, Test: 0.9170\n",
      "Epoch: 45, Loss: 0.2305, Train: 0.9175, Test: 0.9168\n",
      "Epoch: 46, Loss: 0.2292, Train: 0.9192, Test: 0.9186\n",
      "Epoch: 47, Loss: 0.2277, Train: 0.9187, Test: 0.9180\n",
      "Epoch: 48, Loss: 0.2263, Train: 0.9200, Test: 0.9194\n",
      "Epoch: 49, Loss: 0.2251, Train: 0.9201, Test: 0.9195\n",
      "Epoch: 50, Loss: 0.2241, Train: 0.9206, Test: 0.9201\n"
     ]
    }
   ],
   "source": [
    "model = mlp(x.shape[1], 1000, 2)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 1000\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification: Message Passing NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDSSLDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(NDSSLDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['edge_list.csv', 'x_worker.pt', 'y_worker.pt', 'train_mask.pt', 'test_mask.pt', 'edge_attributes.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['NDSSL_graph_full_worker.pt']\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        \n",
    "        ## load the edge list\n",
    "        edge_list = pd.read_csv(self.raw_paths[0], dtype=int) - 2000000 #the node id's start at 2000000, shift these to start at 0         \n",
    "        \n",
    "        ## format the edge list\n",
    "        target_nodes = edge_list.iloc[:,0].values\n",
    "        source_nodes = edge_list.iloc[:,1].values\n",
    "        edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.int64)\n",
    "\n",
    "        ## load the (x,y) formatted data\n",
    "        x = torch.load(self.raw_paths[1], map_location=torch.device('cpu'))\n",
    "        y = torch.load(self.raw_paths[2], map_location=torch.device('cpu'))\n",
    "        train_mask = torch.load(self.raw_paths[3], map_location=torch.device('cpu')) == 1 \n",
    "        test_mask = torch.load(self.raw_paths[4], map_location=torch.device('cpu')) == 1 \n",
    "\n",
    "        ## set the edge weights to be the duration (in hours)\n",
    "        edge_attributes = pd.read_csv(self.raw_paths[5])['duration'].values/3600\n",
    "        duration =  torch.FloatTensor(edge_attributes)\n",
    "        ## previous approaches used the degree:\n",
    "        #row, col = data.edge_index\n",
    "        #data.edge_attr = (1. / degree(col, data.num_nodes)[col]).double()\n",
    "        \n",
    "        ## build the data\n",
    "        data = Data(edge_index=edge_index, x=x, y=y, train_mask=train_mask, test_mask=test_mask)\n",
    "        data.edge_weight = duration\n",
    "        data.train_mask = train_mask\n",
    "        data.test_mask = test_mask\n",
    "        #data.train_mask = torch.cat((torch.ones(n_train, dtype=torch.bool), torch.zeros(n_val, dtype=torch.bool), torch.zeros(n_test, dtype=torch.bool)), dim=0)\n",
    "        #data.test_mask = torch.cat((torch.zeros(n_train, dtype=torch.bool), torch.zeros(n_val, dtype=torch.bool), torch.ones(n_test, dtype=torch.bool)), dim=0)\n",
    "\n",
    "        print(data.__dict__)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "{'x': tensor([[ 1.,  0., 42.,  ...,  3.,  2.,  3.],\n",
      "        [ 0.,  1., 43.,  ...,  3.,  2.,  3.],\n",
      "        [ 1.,  0., 17.,  ...,  3.,  2.,  3.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ...,  8.,  1.,  2.],\n",
      "        [ 0.,  1., 60.,  ...,  1.,  1.,  1.],\n",
      "        [ 0.,  1., 79.,  ...,  1.,  0.,  1.]]), 'edge_index': tensor([[      0,       0,       1,  ..., 1486224, 1378614, 1556530],\n",
      "        [      1,       2,       2,  ..., 1601329, 1601329, 1601329]]), 'edge_attr': None, 'y': tensor([0, 0, 1,  ..., 1, 0, 1]), 'pos': None, 'norm': None, 'face': None, 'train_mask': tensor([True, True, True,  ..., True, True, True]), 'test_mask': tensor([False, False, False,  ..., False, False, False]), 'edge_weight': tensor([10.9161, 12.7494, 12.5828,  ...,  0.0497,  0.1667,  0.1667])}\n",
      "Done!\n",
      "{'x': tensor([[ 1.,  0., 42.,  ...,  3.,  2.,  3.],\n",
      "        [ 0.,  1., 43.,  ...,  3.,  2.,  3.],\n",
      "        [ 1.,  0., 17.,  ...,  3.,  2.,  3.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ...,  8.,  1.,  2.],\n",
      "        [ 0.,  1., 60.,  ...,  1.,  1.,  1.],\n",
      "        [ 0.,  1., 79.,  ...,  1.,  0.,  1.]]), 'edge_index': tensor([[      0,       0,       1,  ..., 1486224, 1378614, 1556530],\n",
      "        [      1,       2,       2,  ..., 1601329, 1601329, 1601329]]), 'edge_attr': None, 'y': tensor([0, 0, 1,  ..., 1, 0, 1]), 'pos': None, 'norm': None, 'face': None, 'train_mask': tensor([True, True, True,  ..., True, True, True]), 'test_mask': tensor([False, False, False,  ..., False, False, False]), 'edge_weight': tensor([10.9161, 12.7494, 12.5828,  ...,  0.0497,  0.1667,  0.1667])}\n"
     ]
    }
   ],
   "source": [
    "## remove old processed files\n",
    "import shutil\n",
    "shutil.rmtree('../../data/NDSSL data/processed')\n",
    "\n",
    "## shuffle the masks\n",
    "dataset = NDSSLDataset('../../data/NDSSL data/')\n",
    "dataset.process()\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': '../../data/NDSSL data',\n",
       " 'transform': None,\n",
       " 'pre_transform': None,\n",
       " 'pre_filter': None,\n",
       " '__indices__': None,\n",
       " 'data': Data(edge_index=[2, 19681821], edge_weight=[19681821], test_mask=[1601330], train_mask=[1601330], x=[1601330, 24], y=[1601330]),\n",
       " 'slices': {'x': tensor([      0, 1601330]),\n",
       "  'edge_index': tensor([       0, 19681821]),\n",
       "  'y': tensor([      0, 1601330]),\n",
       "  'train_mask': tensor([      0, 1601330]),\n",
       "  'test_mask': tensor([      0, 1601330]),\n",
       "  'edge_weight': tensor([       0, 19681821])}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I was initially having a hard time with GraphSAINT, and even for a simple example using the CORA dataset. My computer was crashing after pytorch ate up all the RAM. I believe this is realted to [this issue](https://github.com/rusty1s/pytorch_geometric/issues/1331). The problem was fixed when I set `num_workers=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute GraphSAINT normalization: : 17831210it [00:13, 1343771.41it/s]                            \n"
     ]
    }
   ],
   "source": [
    "loader = GraphSAINTRandomWalkSampler(data, batch_size=6000, walk_length=2,\n",
    "                                     num_steps=5, sample_coverage=10,\n",
    "                                     save_dir=dataset.processed_dir,\n",
    "                                     num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN\n",
    "\n",
    "documentation: https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=GCNConv#torch_geometric.nn.conv.GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels) \n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        #x = F.relu(self.conv3(x, edge_index, edge_weight))\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = x.log_softmax(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 2029002\n"
     ]
    }
   ],
   "source": [
    "model = GCN(1000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        batch_size = out[data.train_mask].shape[0]\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    total_examples_train = correct_train = 0.0\n",
    "    total_examples_test = correct_test = 0.0    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        \n",
    "        ## evaluate the train/test accuracies\n",
    "        train_mask = data.train_mask\n",
    "        test_mask = data.test_mask\n",
    "        total_examples_train += torch.sum(train_mask).item()\n",
    "        total_examples_test += torch.sum(test_mask).item()\n",
    "        correct = (torch.argmax(out, axis=1) == data.y)\n",
    "        \n",
    "        correct_train += torch.sum(correct * train_mask).cpu().item()        \n",
    "        correct_test += torch.sum(correct * test_mask).cpu().item()        \n",
    "    \n",
    "    overall_accuracy_train = correct_train/total_examples_train    \n",
    "    overall_accuracy_test = correct_test/total_examples_test    \n",
    "    \n",
    "    return overall_accuracy_train, overall_accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.7724, Train: 0.4942, Test: 0.5011\n",
      "Epoch: 02, Loss: 0.7498, Train: 0.6080, Test: 0.6041\n",
      "Epoch: 03, Loss: 0.7419, Train: 0.4945, Test: 0.4974\n",
      "Epoch: 04, Loss: 0.7287, Train: 0.6070, Test: 0.6068\n",
      "Epoch: 05, Loss: 0.7200, Train: 0.6067, Test: 0.6043\n",
      "Epoch: 06, Loss: 0.7109, Train: 0.6314, Test: 0.6337\n",
      "Epoch: 07, Loss: 0.7022, Train: 0.6743, Test: 0.6716\n",
      "Epoch: 08, Loss: 0.6934, Train: 0.6863, Test: 0.6846\n",
      "Epoch: 09, Loss: 0.6882, Train: 0.7035, Test: 0.7059\n",
      "Epoch: 10, Loss: 0.6850, Train: 0.7134, Test: 0.7117\n",
      "Epoch: 11, Loss: 0.6751, Train: 0.7074, Test: 0.7051\n",
      "Epoch: 12, Loss: 0.6679, Train: 0.7097, Test: 0.7066\n",
      "Epoch: 13, Loss: 0.6618, Train: 0.7266, Test: 0.7301\n",
      "Epoch: 14, Loss: 0.6545, Train: 0.7293, Test: 0.7261\n",
      "Epoch: 15, Loss: 0.6503, Train: 0.7277, Test: 0.7287\n",
      "Epoch: 16, Loss: 0.6438, Train: 0.7295, Test: 0.7252\n",
      "Epoch: 17, Loss: 0.6392, Train: 0.7309, Test: 0.7380\n",
      "Epoch: 18, Loss: 0.6349, Train: 0.7356, Test: 0.7367\n",
      "Epoch: 19, Loss: 0.6297, Train: 0.7319, Test: 0.7308\n",
      "Epoch: 20, Loss: 0.6249, Train: 0.7202, Test: 0.7255\n",
      "Epoch: 21, Loss: 0.6246, Train: 0.7278, Test: 0.7308\n",
      "Epoch: 22, Loss: 0.6189, Train: 0.7208, Test: 0.7225\n",
      "Epoch: 23, Loss: 0.6133, Train: 0.7229, Test: 0.7249\n",
      "Epoch: 24, Loss: 0.6123, Train: 0.7279, Test: 0.7272\n",
      "Epoch: 25, Loss: 0.6090, Train: 0.7411, Test: 0.7370\n",
      "Epoch: 26, Loss: 0.6071, Train: 0.7394, Test: 0.7356\n",
      "Epoch: 27, Loss: 0.6034, Train: 0.7374, Test: 0.7440\n",
      "Epoch: 28, Loss: 0.5991, Train: 0.7418, Test: 0.7412\n",
      "Epoch: 29, Loss: 0.5935, Train: 0.7408, Test: 0.7394\n",
      "Epoch: 30, Loss: 0.5941, Train: 0.7441, Test: 0.7485\n",
      "Epoch: 31, Loss: 0.5898, Train: 0.7439, Test: 0.7513\n",
      "Epoch: 32, Loss: 0.5909, Train: 0.7521, Test: 0.7567\n",
      "Epoch: 33, Loss: 0.5888, Train: 0.7491, Test: 0.7513\n",
      "Epoch: 34, Loss: 0.5855, Train: 0.7518, Test: 0.7557\n",
      "Epoch: 35, Loss: 0.5854, Train: 0.7484, Test: 0.7535\n",
      "Epoch: 36, Loss: 0.5800, Train: 0.7482, Test: 0.7482\n",
      "Epoch: 37, Loss: 0.5808, Train: 0.7443, Test: 0.7491\n",
      "Epoch: 38, Loss: 0.5794, Train: 0.7452, Test: 0.7486\n",
      "Epoch: 39, Loss: 0.5745, Train: 0.7460, Test: 0.7455\n",
      "Epoch: 40, Loss: 0.5754, Train: 0.7434, Test: 0.7436\n",
      "Epoch: 41, Loss: 0.5729, Train: 0.7432, Test: 0.7470\n",
      "Epoch: 42, Loss: 0.5725, Train: 0.7489, Test: 0.7482\n",
      "Epoch: 43, Loss: 0.5744, Train: 0.7522, Test: 0.7531\n",
      "Epoch: 44, Loss: 0.5724, Train: 0.7538, Test: 0.7497\n",
      "Epoch: 45, Loss: 0.5663, Train: 0.7522, Test: 0.7571\n",
      "Epoch: 46, Loss: 0.5648, Train: 0.7529, Test: 0.7538\n",
      "Epoch: 47, Loss: 0.5657, Train: 0.7486, Test: 0.7469\n",
      "Epoch: 48, Loss: 0.5636, Train: 0.7516, Test: 0.7498\n",
      "Epoch: 49, Loss: 0.5631, Train: 0.7528, Test: 0.7423\n",
      "Epoch: 50, Loss: 0.5625, Train: 0.7475, Test: 0.7494\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels) \n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(2 * hidden_channels, out_channels)\n",
    "        \n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "        self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        \n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "        \n",
    "        #x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "        #x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 4055002\n"
     ]
    }
   ],
   "source": [
    "model = SAGE(1000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        batch_size = out[data.train_mask].shape[0]\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    model.set_aggr('mean')\n",
    "    \n",
    "    total_examples_train = correct_train = 0.0\n",
    "    total_examples_test = correct_test = 0.0    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        \n",
    "        ## evaluate the train/test accuracies\n",
    "        train_mask = data.train_mask\n",
    "        test_mask = data.test_mask\n",
    "        total_examples_train += torch.sum(train_mask).item()\n",
    "        total_examples_test += torch.sum(test_mask).item()\n",
    "        correct = (torch.argmax(out, axis=1) == data.y)\n",
    "        \n",
    "        correct_train += torch.sum(correct * train_mask).cpu().item()        \n",
    "        correct_test += torch.sum(correct * test_mask).cpu().item()        \n",
    "    \n",
    "    overall_accuracy_train = correct_train/total_examples_train    \n",
    "    overall_accuracy_test = correct_test/total_examples_test    \n",
    "    \n",
    "    return overall_accuracy_train, overall_accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 3.9472, Train: 0.4494, Test: 0.4469\n",
      "Epoch: 02, Loss: 3.0674, Train: 0.5302, Test: 0.5323\n",
      "Epoch: 03, Loss: 2.5790, Train: 0.5061, Test: 0.5054\n",
      "Epoch: 04, Loss: 2.2868, Train: 0.5642, Test: 0.5688\n",
      "Epoch: 05, Loss: 2.2038, Train: 0.6902, Test: 0.6899\n",
      "Epoch: 06, Loss: 2.0904, Train: 0.6647, Test: 0.6594\n",
      "Epoch: 07, Loss: 2.0263, Train: 0.6895, Test: 0.6828\n",
      "Epoch: 08, Loss: 1.9475, Train: 0.7395, Test: 0.7465\n",
      "Epoch: 09, Loss: 1.8844, Train: 0.6653, Test: 0.6694\n",
      "Epoch: 10, Loss: 1.8116, Train: 0.7587, Test: 0.7595\n",
      "Epoch: 11, Loss: 1.7820, Train: 0.6705, Test: 0.6687\n",
      "Epoch: 12, Loss: 1.7345, Train: 0.7523, Test: 0.7493\n",
      "Epoch: 13, Loss: 1.6855, Train: 0.7575, Test: 0.7568\n",
      "Epoch: 14, Loss: 1.6934, Train: 0.7663, Test: 0.7653\n",
      "Epoch: 15, Loss: 1.5920, Train: 0.7234, Test: 0.7250\n",
      "Epoch: 16, Loss: 1.5469, Train: 0.7271, Test: 0.7222\n",
      "Epoch: 17, Loss: 1.5146, Train: 0.7775, Test: 0.7795\n",
      "Epoch: 18, Loss: 1.4765, Train: 0.7900, Test: 0.7910\n",
      "Epoch: 19, Loss: 1.4000, Train: 0.7907, Test: 0.7918\n",
      "Epoch: 20, Loss: 1.3890, Train: 0.7644, Test: 0.7670\n",
      "Epoch: 21, Loss: 1.3088, Train: 0.7623, Test: 0.7724\n",
      "Epoch: 22, Loss: 1.2922, Train: 0.7718, Test: 0.7680\n",
      "Epoch: 23, Loss: 1.2891, Train: 0.7733, Test: 0.7719\n",
      "Epoch: 24, Loss: 1.2218, Train: 0.7916, Test: 0.7900\n",
      "Epoch: 25, Loss: 1.2096, Train: 0.7796, Test: 0.7747\n",
      "Epoch: 26, Loss: 1.1598, Train: 0.7951, Test: 0.7926\n",
      "Epoch: 27, Loss: 1.1370, Train: 0.8228, Test: 0.8175\n",
      "Epoch: 28, Loss: 1.1187, Train: 0.8220, Test: 0.8223\n",
      "Epoch: 29, Loss: 1.0633, Train: 0.8203, Test: 0.8160\n",
      "Epoch: 30, Loss: 1.0562, Train: 0.8176, Test: 0.8204\n",
      "Epoch: 31, Loss: 1.0219, Train: 0.8290, Test: 0.8317\n",
      "Epoch: 32, Loss: 0.9872, Train: 0.8238, Test: 0.8258\n",
      "Epoch: 33, Loss: 0.9864, Train: 0.7846, Test: 0.7869\n",
      "Epoch: 34, Loss: 0.9391, Train: 0.7951, Test: 0.7952\n",
      "Epoch: 35, Loss: 0.9175, Train: 0.8381, Test: 0.8448\n",
      "Epoch: 36, Loss: 0.9444, Train: 0.8442, Test: 0.8430\n",
      "Epoch: 37, Loss: 0.9093, Train: 0.8460, Test: 0.8477\n",
      "Epoch: 38, Loss: 0.8809, Train: 0.8453, Test: 0.8466\n",
      "Epoch: 39, Loss: 0.8849, Train: 0.8468, Test: 0.8472\n",
      "Epoch: 40, Loss: 0.8781, Train: 0.8394, Test: 0.8323\n",
      "Epoch: 41, Loss: 0.8565, Train: 0.8183, Test: 0.8180\n",
      "Epoch: 42, Loss: 0.8457, Train: 0.8401, Test: 0.8398\n",
      "Epoch: 43, Loss: 0.8130, Train: 0.8537, Test: 0.8573\n",
      "Epoch: 44, Loss: 0.8268, Train: 0.8509, Test: 0.8511\n",
      "Epoch: 45, Loss: 0.8062, Train: 0.8517, Test: 0.8531\n",
      "Epoch: 46, Loss: 0.7744, Train: 0.8462, Test: 0.8495\n",
      "Epoch: 47, Loss: 0.7917, Train: 0.8532, Test: 0.8568\n",
      "Epoch: 48, Loss: 0.7560, Train: 0.8487, Test: 0.8491\n",
      "Epoch: 49, Loss: 0.7517, Train: 0.8475, Test: 0.8456\n",
      "Epoch: 50, Loss: 0.7402, Train: 0.8417, Test: 0.8449\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch geometric)",
   "language": "python",
   "name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
