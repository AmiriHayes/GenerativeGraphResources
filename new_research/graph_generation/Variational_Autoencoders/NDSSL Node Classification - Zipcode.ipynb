{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDSSL Node Classification - Zipcode\n",
    "\n",
    "**To Do**:\n",
    "- Wy is the GCN/graphSAGE approach so fast/why is the MLP approach so slow?\n",
    "- The MPNN approaches beat the MLP approach, but neither are as good as I expected\n",
    "- I tried using class weights, and results aren't great\n",
    "- does it learn that everyone in a household has the same income?\n",
    "- add more descriptive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.data import GraphSAINTRandomWalkSampler\n",
    "from torch_geometric.utils import get_laplacian, degree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.style as style \n",
    "style.use('seaborn-paper')\n",
    "\n",
    "fontsize = 12\n",
    "plt.rcParams.update({\n",
    "    'font.size': fontsize, \n",
    "    'axes.labelsize': fontsize, \n",
    "    'legend.fontsize': fontsize,\n",
    "    'xtick.labelsize': fontsize,\n",
    "    'ytick.labelsize': fontsize,\n",
    "    'axes.titlesize': fontsize\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#from imports import *\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Convert the node attribute data into an (x,y) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = pd.read_csv('../../data/NDSSL data/raw/node_attributes.csv')\n",
    "\n",
    "## one-hot encode gender\n",
    "gender_index = torch.LongTensor(node_attributes['gender'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "gender_onehot = torch.LongTensor(len(node_attributes), 2)\n",
    "gender_onehot.zero_()\n",
    "gender_onehot = gender_onehot.scatter_(1, gender_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode worker\n",
    "worker_index = torch.LongTensor(node_attributes['worker'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "worker_onehot = torch.LongTensor(len(node_attributes), 2)\n",
    "worker_onehot.zero_()\n",
    "worker_onehot = worker_onehot.scatter_(1, worker_index, 1).type(torch.float32);\n",
    "\n",
    "## map the 117 distinct zipcodes to the integers 0, ..., 116\n",
    "zipcode_original = node_attributes['zipcode'].values\n",
    "zipcode_dict = {i: j for j, i in enumerate(set(zipcode_original))} \n",
    "zipcode_index = torch.LongTensor(np.asarray([zipcode_dict[i] for i in zipcode_original])).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "\n",
    "## one-hot encode zipcode\n",
    "zipcode_onehot = torch.LongTensor(len(node_attributes), len(zipcode_dict))\n",
    "zipcode_onehot.zero_()\n",
    "zipcode_onehot = zipcode_onehot.scatter_(1, zipcode_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode household income\n",
    "household_income_index = torch.LongTensor(node_attributes['household_income'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "household_income_onehot = torch.LongTensor(len(node_attributes), 14)\n",
    "household_income_onehot.zero_()\n",
    "household_income_onehot = household_income_onehot.scatter_(1, household_income_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode relationship\n",
    "relationship_index = torch.LongTensor(node_attributes['relationship'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "relationship_onehot = torch.LongTensor(len(node_attributes), 4)\n",
    "relationship_onehot.zero_()\n",
    "relationship_onehot = relationship_onehot.scatter_(1, relationship_index, 1).type(torch.float32);\n",
    "\n",
    "age = torch.FloatTensor(node_attributes['age'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_size = torch.FloatTensor(node_attributes['household_size'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_workers = torch.FloatTensor(node_attributes['household_workers'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_vehicles = torch.FloatTensor(node_attributes['household_vehicles'].values).reshape(len(node_attributes), 1).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1601330, 26]) torch.float32\n",
      "torch.Size([1601330]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.cat((gender_onehot, age, relationship_onehot, worker_onehot, household_income_onehot, household_size, household_workers, household_vehicles), dim=1)\n",
    "y = zipcode_index[:,0]\n",
    "\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct households: 632626\n",
      "number of training households: 506100\n",
      "number of testing households: 126526\n",
      "size of training set: 1281023\n",
      "size of testing set: 320307\n",
      "percent train: 0.80\n"
     ]
    }
   ],
   "source": [
    "## create a train/test split by household\n",
    "household_ids = list(set(list(node_attributes['household_id'])))\n",
    "np.random.shuffle(household_ids)\n",
    "\n",
    "ntrain_households = int(0.8 * len(household_ids))\n",
    "households_train = household_ids[:ntrain_households]\n",
    "households_test = household_ids[ntrain_households:]\n",
    "print('number of distinct households: %i' %len(household_ids))\n",
    "print('number of training households: %i' %len(households_train))\n",
    "print('number of testing households: %i' %len(households_test))\n",
    "\n",
    "train_mask = torch.IntTensor(node_attributes['household_id'].isin(households_train)).type(torch.int64)\n",
    "test_mask = torch.IntTensor(node_attributes['household_id'].isin(households_test)).type(torch.int64)\n",
    "train_idx = np.arange(len(node_attributes))[train_mask == 1]\n",
    "test_idx = np.arange(len(node_attributes))[test_mask == 1]\n",
    "\n",
    "print('size of training set: %i' %torch.sum(train_mask).item())\n",
    "print('size of testing set: %i' %torch.sum(test_mask).item())\n",
    "print('percent train: %.2f' %(torch.sum(train_mask).item()/len(train_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, '../../data/NDSSL data/raw/x.pt')\n",
    "torch.save(y, '../../data/NDSSL data/raw/y.pt')\n",
    "torch.save(train_mask, '../../data/NDSSL data/raw/train_mask.pt')\n",
    "torch.save(test_mask, '../../data/NDSSL data/raw/test_mask.pt')\n",
    "torch.save(train_idx, '../../data/NDSSL data/raw/train_idx.pt')\n",
    "torch.save(test_idx, '../../data/NDSSL data/raw/test_idx.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification: MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = torch.utils.data.TensorDataset(x, y)\n",
    "train_set = torch.utils.data.Subset(full_set, train_idx)\n",
    "test_set = torch.utils.data.Subset(full_set, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(mlp, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)    \n",
    "        x = x.log_softmax(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        loss = F.nll_loss(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_batch.shape[0]\n",
    "        total_examples += x_batch.shape[0]\n",
    "    \n",
    "    return total_loss/total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    ## train accuracy\n",
    "    total_examples = correct = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        total_examples += out.shape[0]\n",
    "        correct += torch.sum(torch.argmax(out, axis=1) == y_batch).cpu().item()\n",
    "    train_accuracy = correct/total_examples\n",
    "    \n",
    "    ## test accuracy\n",
    "    total_examples = correct = 0.0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        total_examples += x_batch.shape[0]\n",
    "        correct += torch.sum(torch.argmax(out, axis=1) == y_batch).cpu().item()\n",
    "    test_accuracy = correct/total_examples\n",
    "    \n",
    "    return train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: why is this so slow, even with a GPU?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 1145117\n",
      "Epoch: 01, Loss: 4.7883, Train: 0.0266, Test: 0.0260\n",
      "Epoch: 02, Loss: 4.5907, Train: 0.0257, Test: 0.0255\n",
      "Epoch: 03, Loss: 4.5413, Train: 0.0264, Test: 0.0262\n",
      "Epoch: 04, Loss: 4.5127, Train: 0.0267, Test: 0.0263\n",
      "Epoch: 05, Loss: 4.4949, Train: 0.0266, Test: 0.0262\n",
      "Epoch: 06, Loss: 4.4833, Train: 0.0270, Test: 0.0265\n",
      "Epoch: 07, Loss: 4.4746, Train: 0.0271, Test: 0.0267\n",
      "Epoch: 08, Loss: 4.4684, Train: 0.0283, Test: 0.0277\n",
      "Epoch: 09, Loss: 4.4641, Train: 0.0290, Test: 0.0285\n",
      "Epoch: 10, Loss: 4.4610, Train: 0.0299, Test: 0.0297\n",
      "Epoch: 11, Loss: 4.4578, Train: 0.0296, Test: 0.0295\n",
      "Epoch: 12, Loss: 4.4554, Train: 0.0273, Test: 0.0266\n",
      "Epoch: 13, Loss: 4.4536, Train: 0.0290, Test: 0.0283\n",
      "Epoch: 14, Loss: 4.4518, Train: 0.0285, Test: 0.0279\n",
      "Epoch: 15, Loss: 4.4502, Train: 0.0299, Test: 0.0295\n",
      "Epoch: 16, Loss: 4.4488, Train: 0.0300, Test: 0.0295\n",
      "Epoch: 17, Loss: 4.4473, Train: 0.0293, Test: 0.0291\n",
      "Epoch: 18, Loss: 4.4466, Train: 0.0292, Test: 0.0291\n",
      "Epoch: 19, Loss: 4.4451, Train: 0.0292, Test: 0.0287\n",
      "Epoch: 20, Loss: 4.4443, Train: 0.0295, Test: 0.0291\n",
      "Epoch: 21, Loss: 4.4433, Train: 0.0294, Test: 0.0291\n",
      "Epoch: 22, Loss: 4.4431, Train: 0.0295, Test: 0.0290\n",
      "Epoch: 23, Loss: 4.4419, Train: 0.0292, Test: 0.0286\n",
      "Epoch: 24, Loss: 4.4411, Train: 0.0294, Test: 0.0292\n",
      "Epoch: 25, Loss: 4.4405, Train: 0.0293, Test: 0.0290\n",
      "Epoch: 26, Loss: 4.4400, Train: 0.0297, Test: 0.0293\n",
      "Epoch: 27, Loss: 4.4389, Train: 0.0294, Test: 0.0292\n",
      "Epoch: 28, Loss: 4.4386, Train: 0.0294, Test: 0.0295\n"
     ]
    }
   ],
   "source": [
    "model = mlp(x.shape[1], 1000, 117)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 1000\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification: Message Passing NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDSSLDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(NDSSLDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['edge_list.csv', 'x_zipcode.pt', 'y_zipcode.pt', 'train_mask.pt', 'test_mask.pt', 'edge_attributes.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['NDSSL_graph_full_worker.pt']\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        \n",
    "        ## load the edge list\n",
    "        edge_list = pd.read_csv(self.raw_paths[0], dtype=int) - 2000000 #the node id's start at 2000000, shift these to start at 0         \n",
    "        \n",
    "        ## format the edge list\n",
    "        target_nodes = edge_list.iloc[:,0].values\n",
    "        source_nodes = edge_list.iloc[:,1].values\n",
    "        edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.int64)\n",
    "\n",
    "        ## load the (x,y) formatted data\n",
    "        x = torch.load(self.raw_paths[1], map_location=torch.device('cpu'))\n",
    "        y = torch.load(self.raw_paths[2], map_location=torch.device('cpu'))\n",
    "        train_mask = torch.load(self.raw_paths[3], map_location=torch.device('cpu')) == 1 \n",
    "        test_mask = torch.load(self.raw_paths[4], map_location=torch.device('cpu')) == 1 \n",
    "\n",
    "        ## set the edge weights to be the duration (in hours)\n",
    "        edge_attributes = pd.read_csv(self.raw_paths[5])['duration'].values/3600\n",
    "        duration =  torch.FloatTensor(edge_attributes)\n",
    "        ## previous approaches used the degree:\n",
    "        #row, col = data.edge_index\n",
    "        #data.edge_attr = (1. / degree(col, data.num_nodes)[col]).double()\n",
    "        \n",
    "        ## build the data\n",
    "        data = Data(edge_index=edge_index, x=x, y=y, train_mask=train_mask, test_mask=test_mask)\n",
    "        data.edge_weight = duration\n",
    "        data.train_mask = train_mask\n",
    "        data.test_mask = test_mask\n",
    "        #data.train_mask = torch.cat((torch.ones(n_train, dtype=torch.bool), torch.zeros(n_val, dtype=torch.bool), torch.zeros(n_test, dtype=torch.bool)), dim=0)\n",
    "        #data.test_mask = torch.cat((torch.zeros(n_train, dtype=torch.bool), torch.zeros(n_val, dtype=torch.bool), torch.ones(n_test, dtype=torch.bool)), dim=0)\n",
    "\n",
    "        print(data.__dict__)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "{'x': tensor([[ 1.,  0., 42.,  ...,  3.,  2.,  3.],\n",
      "        [ 0.,  1., 43.,  ...,  3.,  2.,  3.],\n",
      "        [ 1.,  0., 17.,  ...,  3.,  2.,  3.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ...,  8.,  1.,  2.],\n",
      "        [ 0.,  1., 60.,  ...,  1.,  1.,  1.],\n",
      "        [ 0.,  1., 79.,  ...,  1.,  0.,  1.]]), 'edge_index': tensor([[      0,       0,       1,  ..., 1486224, 1378614, 1556530],\n",
      "        [      1,       2,       2,  ..., 1601329, 1601329, 1601329]]), 'edge_attr': None, 'y': tensor([79, 79, 79,  ..., 36, 45, 36]), 'pos': None, 'norm': None, 'face': None, 'train_mask': tensor([True, True, True,  ..., True, True, True]), 'test_mask': tensor([False, False, False,  ..., False, False, False]), 'edge_weight': tensor([10.9161, 12.7494, 12.5828,  ...,  0.0497,  0.1667,  0.1667])}\n",
      "Done!\n",
      "{'x': tensor([[ 1.,  0., 42.,  ...,  3.,  2.,  3.],\n",
      "        [ 0.,  1., 43.,  ...,  3.,  2.,  3.],\n",
      "        [ 1.,  0., 17.,  ...,  3.,  2.,  3.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ...,  8.,  1.,  2.],\n",
      "        [ 0.,  1., 60.,  ...,  1.,  1.,  1.],\n",
      "        [ 0.,  1., 79.,  ...,  1.,  0.,  1.]]), 'edge_index': tensor([[      0,       0,       1,  ..., 1486224, 1378614, 1556530],\n",
      "        [      1,       2,       2,  ..., 1601329, 1601329, 1601329]]), 'edge_attr': None, 'y': tensor([79, 79, 79,  ..., 36, 45, 36]), 'pos': None, 'norm': None, 'face': None, 'train_mask': tensor([True, True, True,  ..., True, True, True]), 'test_mask': tensor([False, False, False,  ..., False, False, False]), 'edge_weight': tensor([10.9161, 12.7494, 12.5828,  ...,  0.0497,  0.1667,  0.1667])}\n"
     ]
    }
   ],
   "source": [
    "## remove old processed files\n",
    "import shutil\n",
    "shutil.rmtree('../../data/NDSSL data/processed')\n",
    "\n",
    "## shuffle the masks\n",
    "dataset = NDSSLDataset('../../data/NDSSL data/')\n",
    "dataset.process()\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': '../../data/NDSSL data',\n",
       " 'transform': None,\n",
       " 'pre_transform': None,\n",
       " 'pre_filter': None,\n",
       " '__indices__': None,\n",
       " 'data': Data(edge_index=[2, 19681821], edge_weight=[19681821], test_mask=[1601330], train_mask=[1601330], x=[1601330, 26], y=[1601330]),\n",
       " 'slices': {'x': tensor([      0, 1601330]),\n",
       "  'edge_index': tensor([       0, 19681821]),\n",
       "  'y': tensor([      0, 1601330]),\n",
       "  'train_mask': tensor([      0, 1601330]),\n",
       "  'test_mask': tensor([      0, 1601330]),\n",
       "  'edge_weight': tensor([       0, 19681821])}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I was initially having a hard time with GraphSAINT, and even for a simple example using the CORA dataset. My computer was crashing after pytorch ate up all the RAM. I believe this is realted to [this issue](https://github.com/rusty1s/pytorch_geometric/issues/1331). The problem was fixed when I set `num_workers=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute GraphSAINT normalization: : 17831555it [00:12, 1384473.80it/s]                            \n"
     ]
    }
   ],
   "source": [
    "loader = GraphSAINTRandomWalkSampler(data, batch_size=6000, walk_length=2,\n",
    "                                     num_steps=5, sample_coverage=10,\n",
    "                                     save_dir=dataset.processed_dir,\n",
    "                                     num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN\n",
    "\n",
    "documentation: https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=GCNConv#torch_geometric.nn.conv.GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels) \n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        #x = F.relu(self.conv3(x, edge_index, edge_weight))\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = x.log_softmax(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 2146117\n"
     ]
    }
   ],
   "source": [
    "model = GCN(1000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        batch_size = out[data.train_mask].shape[0]\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    total_examples_train = correct_train = 0.0\n",
    "    total_examples_test = correct_test = 0.0    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        \n",
    "        ## evaluate the train/test accuracies\n",
    "        train_mask = data.train_mask\n",
    "        test_mask = data.test_mask\n",
    "        total_examples_train += torch.sum(train_mask).item()\n",
    "        total_examples_test += torch.sum(test_mask).item()\n",
    "        correct = (torch.argmax(out, axis=1) == data.y)\n",
    "        \n",
    "        correct_train += torch.sum(correct * train_mask).cpu().item()        \n",
    "        correct_test += torch.sum(correct * test_mask).cpu().item()        \n",
    "    \n",
    "    overall_accuracy_train = correct_train/total_examples_train    \n",
    "    overall_accuracy_test = correct_test/total_examples_test    \n",
    "    \n",
    "    return overall_accuracy_train, overall_accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 4.9592, Train: 0.0254, Test: 0.0271\n",
      "Epoch: 02, Loss: 4.7648, Train: 0.0197, Test: 0.0186\n",
      "Epoch: 03, Loss: 4.7176, Train: 0.0205, Test: 0.0206\n",
      "Epoch: 04, Loss: 4.6975, Train: 0.0269, Test: 0.0242\n",
      "Epoch: 05, Loss: 4.6705, Train: 0.0249, Test: 0.0252\n",
      "Epoch: 06, Loss: 4.6605, Train: 0.0267, Test: 0.0294\n",
      "Epoch: 07, Loss: 4.6447, Train: 0.0273, Test: 0.0247\n",
      "Epoch: 08, Loss: 4.6374, Train: 0.0279, Test: 0.0278\n",
      "Epoch: 09, Loss: 4.6297, Train: 0.0285, Test: 0.0285\n",
      "Epoch: 10, Loss: 4.6212, Train: 0.0283, Test: 0.0243\n",
      "Epoch: 11, Loss: 4.6172, Train: 0.0288, Test: 0.0281\n",
      "Epoch: 12, Loss: 4.6097, Train: 0.0302, Test: 0.0281\n",
      "Epoch: 13, Loss: 4.6046, Train: 0.0290, Test: 0.0278\n",
      "Epoch: 14, Loss: 4.5974, Train: 0.0290, Test: 0.0272\n",
      "Epoch: 15, Loss: 4.5952, Train: 0.0293, Test: 0.0280\n",
      "Epoch: 16, Loss: 4.5881, Train: 0.0290, Test: 0.0257\n",
      "Epoch: 17, Loss: 4.5864, Train: 0.0285, Test: 0.0302\n",
      "Epoch: 18, Loss: 4.5815, Train: 0.0286, Test: 0.0279\n",
      "Epoch: 19, Loss: 4.5785, Train: 0.0287, Test: 0.0293\n",
      "Epoch: 20, Loss: 4.5782, Train: 0.0277, Test: 0.0305\n",
      "Epoch: 21, Loss: 4.5710, Train: 0.0299, Test: 0.0310\n",
      "Epoch: 22, Loss: 4.5707, Train: 0.0297, Test: 0.0303\n",
      "Epoch: 23, Loss: 4.5676, Train: 0.0285, Test: 0.0287\n",
      "Epoch: 24, Loss: 4.5649, Train: 0.0292, Test: 0.0297\n",
      "Epoch: 25, Loss: 4.5640, Train: 0.0289, Test: 0.0263\n",
      "Epoch: 26, Loss: 4.5581, Train: 0.0281, Test: 0.0263\n",
      "Epoch: 27, Loss: 4.5588, Train: 0.0309, Test: 0.0286\n",
      "Epoch: 28, Loss: 4.5540, Train: 0.0309, Test: 0.0309\n",
      "Epoch: 29, Loss: 4.5538, Train: 0.0288, Test: 0.0280\n",
      "Epoch: 30, Loss: 4.5527, Train: 0.0309, Test: 0.0297\n",
      "Epoch: 31, Loss: 4.5509, Train: 0.0304, Test: 0.0287\n",
      "Epoch: 32, Loss: 4.5474, Train: 0.0314, Test: 0.0304\n",
      "Epoch: 33, Loss: 4.5449, Train: 0.0311, Test: 0.0302\n",
      "Epoch: 34, Loss: 4.5437, Train: 0.0294, Test: 0.0305\n",
      "Epoch: 35, Loss: 4.5435, Train: 0.0306, Test: 0.0273\n",
      "Epoch: 36, Loss: 4.5388, Train: 0.0309, Test: 0.0299\n",
      "Epoch: 37, Loss: 4.5386, Train: 0.0295, Test: 0.0285\n",
      "Epoch: 38, Loss: 4.5407, Train: 0.0266, Test: 0.0257\n",
      "Epoch: 39, Loss: 4.5385, Train: 0.0309, Test: 0.0304\n",
      "Epoch: 40, Loss: 4.5353, Train: 0.0303, Test: 0.0281\n",
      "Epoch: 41, Loss: 4.5314, Train: 0.0304, Test: 0.0291\n",
      "Epoch: 42, Loss: 4.5295, Train: 0.0319, Test: 0.0310\n",
      "Epoch: 43, Loss: 4.5313, Train: 0.0323, Test: 0.0317\n",
      "Epoch: 44, Loss: 4.5249, Train: 0.0301, Test: 0.0289\n",
      "Epoch: 45, Loss: 4.5282, Train: 0.0321, Test: 0.0299\n",
      "Epoch: 46, Loss: 4.5224, Train: 0.0317, Test: 0.0276\n",
      "Epoch: 47, Loss: 4.5189, Train: 0.0313, Test: 0.0311\n",
      "Epoch: 48, Loss: 4.5198, Train: 0.0323, Test: 0.0324\n",
      "Epoch: 49, Loss: 4.5181, Train: 0.0310, Test: 0.0293\n",
      "Epoch: 50, Loss: 4.5172, Train: 0.0317, Test: 0.0308\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels) \n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(2 * hidden_channels, out_channels)\n",
    "        \n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "        self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        \n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "        \n",
    "        #x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "        #x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 4289117\n"
     ]
    }
   ],
   "source": [
    "model = SAGE(1000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        batch_size = out[data.train_mask].shape[0]\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    model.set_aggr('mean')\n",
    "    \n",
    "    total_examples_train = correct_train = 0.0\n",
    "    total_examples_test = correct_test = 0.0    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        \n",
    "        ## evaluate the train/test accuracies\n",
    "        train_mask = data.train_mask\n",
    "        test_mask = data.test_mask\n",
    "        total_examples_train += torch.sum(train_mask).item()\n",
    "        total_examples_test += torch.sum(test_mask).item()\n",
    "        correct = (torch.argmax(out, axis=1) == data.y)\n",
    "        \n",
    "        correct_train += torch.sum(correct * train_mask).cpu().item()        \n",
    "        correct_test += torch.sum(correct * test_mask).cpu().item()        \n",
    "    \n",
    "    overall_accuracy_train = correct_train/total_examples_train    \n",
    "    overall_accuracy_test = correct_test/total_examples_test    \n",
    "    \n",
    "    return overall_accuracy_train, overall_accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 12.3795, Train: 0.0194, Test: 0.0212\n",
      "Epoch: 02, Loss: 9.7682, Train: 0.0187, Test: 0.0193\n",
      "Epoch: 03, Loss: 9.1737, Train: 0.0203, Test: 0.0203\n",
      "Epoch: 04, Loss: 8.7102, Train: 0.0197, Test: 0.0198\n",
      "Epoch: 05, Loss: 8.1069, Train: 0.0185, Test: 0.0190\n",
      "Epoch: 06, Loss: 7.7543, Train: 0.0225, Test: 0.0214\n",
      "Epoch: 07, Loss: 7.4228, Train: 0.0220, Test: 0.0211\n",
      "Epoch: 08, Loss: 7.1407, Train: 0.0219, Test: 0.0225\n",
      "Epoch: 09, Loss: 6.8851, Train: 0.0214, Test: 0.0223\n",
      "Epoch: 10, Loss: 6.7615, Train: 0.0213, Test: 0.0221\n",
      "Epoch: 11, Loss: 6.6129, Train: 0.0224, Test: 0.0226\n",
      "Epoch: 12, Loss: 6.5228, Train: 0.0224, Test: 0.0241\n",
      "Epoch: 13, Loss: 6.4420, Train: 0.0233, Test: 0.0222\n",
      "Epoch: 14, Loss: 6.3383, Train: 0.0251, Test: 0.0253\n",
      "Epoch: 15, Loss: 6.3408, Train: 0.0260, Test: 0.0251\n",
      "Epoch: 16, Loss: 6.2682, Train: 0.0257, Test: 0.0259\n",
      "Epoch: 17, Loss: 6.2273, Train: 0.0240, Test: 0.0263\n",
      "Epoch: 18, Loss: 6.1893, Train: 0.0274, Test: 0.0282\n",
      "Epoch: 19, Loss: 6.1466, Train: 0.0248, Test: 0.0253\n",
      "Epoch: 20, Loss: 6.1534, Train: 0.0279, Test: 0.0257\n",
      "Epoch: 21, Loss: 6.1221, Train: 0.0269, Test: 0.0273\n",
      "Epoch: 22, Loss: 6.0896, Train: 0.0270, Test: 0.0285\n",
      "Epoch: 23, Loss: 6.0643, Train: 0.0264, Test: 0.0256\n",
      "Epoch: 24, Loss: 6.0362, Train: 0.0270, Test: 0.0268\n",
      "Epoch: 25, Loss: 6.0285, Train: 0.0278, Test: 0.0254\n",
      "Epoch: 26, Loss: 5.9907, Train: 0.0265, Test: 0.0263\n",
      "Epoch: 27, Loss: 5.9762, Train: 0.0285, Test: 0.0281\n",
      "Epoch: 28, Loss: 5.9537, Train: 0.0270, Test: 0.0271\n",
      "Epoch: 29, Loss: 5.9624, Train: 0.0284, Test: 0.0293\n",
      "Epoch: 30, Loss: 5.9149, Train: 0.0288, Test: 0.0292\n",
      "Epoch: 31, Loss: 5.8821, Train: 0.0271, Test: 0.0258\n",
      "Epoch: 32, Loss: 5.8754, Train: 0.0288, Test: 0.0266\n",
      "Epoch: 33, Loss: 5.8450, Train: 0.0286, Test: 0.0287\n",
      "Epoch: 34, Loss: 5.8064, Train: 0.0290, Test: 0.0277\n",
      "Epoch: 35, Loss: 5.8221, Train: 0.0291, Test: 0.0253\n",
      "Epoch: 36, Loss: 5.7901, Train: 0.0271, Test: 0.0257\n",
      "Epoch: 37, Loss: 5.7606, Train: 0.0296, Test: 0.0264\n",
      "Epoch: 38, Loss: 5.7379, Train: 0.0284, Test: 0.0284\n",
      "Epoch: 39, Loss: 5.7156, Train: 0.0309, Test: 0.0300\n",
      "Epoch: 40, Loss: 5.7000, Train: 0.0269, Test: 0.0276\n",
      "Epoch: 41, Loss: 5.6858, Train: 0.0293, Test: 0.0317\n",
      "Epoch: 42, Loss: 5.6695, Train: 0.0293, Test: 0.0321\n",
      "Epoch: 43, Loss: 5.6391, Train: 0.0300, Test: 0.0297\n",
      "Epoch: 44, Loss: 5.6369, Train: 0.0293, Test: 0.0284\n",
      "Epoch: 45, Loss: 5.6323, Train: 0.0280, Test: 0.0297\n",
      "Epoch: 46, Loss: 5.6063, Train: 0.0289, Test: 0.0274\n",
      "Epoch: 47, Loss: 5.5783, Train: 0.0292, Test: 0.0276\n",
      "Epoch: 48, Loss: 5.5525, Train: 0.0302, Test: 0.0292\n",
      "Epoch: 49, Loss: 5.5369, Train: 0.0285, Test: 0.0274\n",
      "Epoch: 50, Loss: 5.5318, Train: 0.0302, Test: 0.0304\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch geometric)",
   "language": "python",
   "name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
