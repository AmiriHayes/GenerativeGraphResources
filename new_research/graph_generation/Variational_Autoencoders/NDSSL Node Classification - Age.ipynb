{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDSSL Node Classification - Age\n",
    "\n",
    "**To Do**:\n",
    "- Wy is the GCN/graphSAGE approach so fast/why is the MLP approach so slow?\n",
    "- The MPNN approaches beat the MLP approach, but neither are as good as I expected\n",
    "- I tried using class weights, and results aren't great\n",
    "- does it learn that everyone in a household has the same income?\n",
    "- add more descriptive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.data import GraphSAINTRandomWalkSampler\n",
    "from torch_geometric.utils import get_laplacian, degree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.style as style \n",
    "style.use('seaborn-paper')\n",
    "\n",
    "fontsize = 12\n",
    "plt.rcParams.update({\n",
    "    'font.size': fontsize, \n",
    "    'axes.labelsize': fontsize, \n",
    "    'legend.fontsize': fontsize,\n",
    "    'xtick.labelsize': fontsize,\n",
    "    'ytick.labelsize': fontsize,\n",
    "    'axes.titlesize': fontsize\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#from imports import *\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Convert the node attribute data into an (x,y) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = pd.read_csv('../../data/NDSSL data/raw/node_attributes.csv')\n",
    "\n",
    "## one-hot encode gender\n",
    "gender_index = torch.LongTensor(node_attributes['gender'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "gender_onehot = torch.LongTensor(len(node_attributes), 2)\n",
    "gender_onehot.zero_()\n",
    "gender_onehot = gender_onehot.scatter_(1, gender_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode worker\n",
    "worker_index = torch.LongTensor(node_attributes['worker'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "worker_onehot = torch.LongTensor(len(node_attributes), 2)\n",
    "worker_onehot.zero_()\n",
    "worker_onehot = worker_onehot.scatter_(1, worker_index, 1).type(torch.float32);\n",
    "\n",
    "## map the 117 distinct zipcodes to the integers 0, ..., 116\n",
    "zipcode_original = node_attributes['zipcode'].values\n",
    "zipcode_dict = {i: j for j, i in enumerate(set(zipcode_original))} \n",
    "zipcode_index = torch.LongTensor(np.asarray([zipcode_dict[i] for i in zipcode_original])).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "\n",
    "## one-hot encode zipcode\n",
    "zipcode_onehot = torch.LongTensor(len(node_attributes), len(zipcode_dict))\n",
    "zipcode_onehot.zero_()\n",
    "zipcode_onehot = zipcode_onehot.scatter_(1, zipcode_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode household income\n",
    "household_income_index = torch.LongTensor(node_attributes['household_income'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "household_income_onehot = torch.LongTensor(len(node_attributes), 14)\n",
    "household_income_onehot.zero_()\n",
    "household_income_onehot = household_income_onehot.scatter_(1, household_income_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode relationship\n",
    "relationship_index = torch.LongTensor(node_attributes['relationship'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "relationship_onehot = torch.LongTensor(len(node_attributes), 4)\n",
    "relationship_onehot.zero_()\n",
    "relationship_onehot = relationship_onehot.scatter_(1, relationship_index, 1).type(torch.float32);\n",
    "\n",
    "age = torch.FloatTensor(node_attributes['age'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_size = torch.FloatTensor(node_attributes['household_size'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_workers = torch.FloatTensor(node_attributes['household_workers'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_vehicles = torch.FloatTensor(node_attributes['household_vehicles'].values).reshape(len(node_attributes), 1).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin the ages to convert the continuous range of ages into integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(90.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(age))\n",
    "print(torch.min(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "age_binned = (age - 0e-4)/9\n",
    "age_binned = age_binned.type(torch.int64)\n",
    "print(torch.max(age_binned))\n",
    "print(torch.min(age_binned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the histograms look the same when the appropriate number of bins is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x316.8 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.hist(age_binned.numpy()[:,0], bins=10, alpha=1, color='k', histtype='step', linewidth=2)\n",
    "plt.hist(age_binned.numpy()[:,0], bins=10, alpha=0.75, linewidth=2, label='binned')\n",
    "\n",
    "plt.hist(age.numpy()[:,0]/9, bins=10, alpha=1, color='k', histtype='step', linewidth=2)\n",
    "plt.hist(age.numpy()[:,0]/9, bins=10, alpha=0.75, linewidth=2, label='float')\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.xlim(0,9)\n",
    "plt.title(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1601330, 25]) torch.float32\n",
      "torch.Size([1601330, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.cat((gender_onehot, relationship_onehot, worker_onehot, household_income_onehot, household_size, household_workers, household_vehicles), dim=1)\n",
    "y = age #age_binned[:,0]\n",
    "\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct households: 632626\n",
      "number of training households: 506100\n",
      "number of testing households: 126526\n",
      "size of training set: 1279922\n",
      "size of testing set: 321408\n",
      "percent train: 0.80\n"
     ]
    }
   ],
   "source": [
    "## create a train/test split by household\n",
    "household_ids = list(set(list(node_attributes['household_id'])))\n",
    "np.random.shuffle(household_ids)\n",
    "\n",
    "ntrain_households = int(0.8 * len(household_ids))\n",
    "households_train = household_ids[:ntrain_households]\n",
    "households_test = household_ids[ntrain_households:]\n",
    "print('number of distinct households: %i' %len(household_ids))\n",
    "print('number of training households: %i' %len(households_train))\n",
    "print('number of testing households: %i' %len(households_test))\n",
    "\n",
    "train_mask = torch.IntTensor(node_attributes['household_id'].isin(households_train)).type(torch.int64)\n",
    "test_mask = torch.IntTensor(node_attributes['household_id'].isin(households_test)).type(torch.int64)\n",
    "train_idx = np.arange(len(node_attributes))[train_mask == 1]\n",
    "test_idx = np.arange(len(node_attributes))[test_mask == 1]\n",
    "\n",
    "print('size of training set: %i' %torch.sum(train_mask).item())\n",
    "print('size of testing set: %i' %torch.sum(test_mask).item())\n",
    "print('percent train: %.2f' %(torch.sum(train_mask).item()/len(train_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, '../../data/NDSSL data/raw/x.pt')\n",
    "torch.save(y, '../../data/NDSSL data/raw/y.pt')\n",
    "torch.save(train_mask, '../../data/NDSSL data/raw/train_mask.pt')\n",
    "torch.save(test_mask, '../../data/NDSSL data/raw/test_mask.pt')\n",
    "torch.save(train_idx, '../../data/NDSSL data/raw/train_idx.pt')\n",
    "torch.save(test_idx, '../../data/NDSSL data/raw/test_idx.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification: MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = torch.utils.data.TensorDataset(x, y)\n",
    "train_set = torch.utils.data.Subset(full_set, train_idx)\n",
    "test_set = torch.utils.data.Subset(full_set, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(mlp, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)    \n",
    "        #x = x.log_softmax(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        #loss = F.nll_loss(out, y_batch) #torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "        loss = nn.MSELoss()(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_batch.shape[0]\n",
    "        total_examples += x_batch.shape[0]\n",
    "    \n",
    "    return total_loss/total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_accuracy():\n",
    "    model.eval()\n",
    "    \n",
    "    ## train accuracy\n",
    "    total_examples = correct = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        total_examples += out.shape[0]\n",
    "        correct += torch.sum(torch.argmax(out, axis=1) == y_batch).cpu().item()\n",
    "    train_accuracy = correct/total_examples\n",
    "    \n",
    "    ## test accuracy\n",
    "    total_examples = correct = 0.0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        total_examples += x_batch.shape[0]\n",
    "        correct += torch.sum(torch.argmax(out, axis=1) == y_batch).cpu().item()\n",
    "    test_accuracy = correct/total_examples\n",
    "    \n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loss():\n",
    "    model.eval()\n",
    "    \n",
    "    ## train loss\n",
    "    train_loss = 0.0\n",
    "    train_examples = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        train_examples += x_batch.shape[0]\n",
    "        train_loss += nn.MSELoss()(out, y_batch) * x_batch.shape[0]\n",
    "    train_loss = train_loss/train_examples\n",
    "    \n",
    "    ## test loss\n",
    "    test_loss = 0.0\n",
    "    test_examples = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        test_examples += x_batch.shape[0]\n",
    "        test_loss += nn.MSELoss()(out, y_batch) * x_batch.shape[0]\n",
    "    test_loss = test_loss/test_examples\n",
    "    \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: why is this so slow, even with a GPU?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 1038011\n",
      "Epoch: 01, Loss: 164.1530, Train: 118.4091, Test: 118.4091\n",
      "Epoch: 02, Loss: 126.7217, Train: 116.6967, Test: 116.6967\n",
      "Epoch: 03, Loss: 122.2368, Train: 114.8781, Test: 114.8781\n",
      "Epoch: 04, Loss: 118.4757, Train: 108.6051, Test: 108.6051\n",
      "Epoch: 05, Loss: 115.5760, Train: 105.6334, Test: 105.6334\n",
      "Epoch: 06, Loss: 113.8171, Train: 104.5162, Test: 104.5162\n",
      "Epoch: 07, Loss: 112.5084, Train: 103.5413, Test: 103.5413\n",
      "Epoch: 08, Loss: 111.5861, Train: 103.1921, Test: 103.1921\n",
      "Epoch: 09, Loss: 110.6563, Train: 102.4255, Test: 102.4256\n",
      "Epoch: 10, Loss: 110.0457, Train: 102.4449, Test: 102.4449\n",
      "Epoch: 11, Loss: 109.4266, Train: 101.6750, Test: 101.6750\n"
     ]
    }
   ],
   "source": [
    "model = mlp(x.shape[1], 1000, len(set(age_binned.numpy()[:,0])))\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 1000\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    loss = train()\n",
    "    #accs = test_accuracy()\n",
    "    losses = test_loss()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {losses[0]:.4f}, 'f'Test: {losses[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification: Message Passing NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDSSLDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(NDSSLDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['edge_list.csv', 'x.pt', 'y.pt', 'train_mask.pt', 'test_mask.pt', 'edge_attributes.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['NDSSL_graph_full_worker.pt']\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        \n",
    "        ## load the edge list\n",
    "        edge_list = pd.read_csv(self.raw_paths[0], dtype=int) - 2000000 #the node id's start at 2000000, shift these to start at 0         \n",
    "        \n",
    "        ## format the edge list\n",
    "        target_nodes = edge_list.iloc[:,0].values\n",
    "        source_nodes = edge_list.iloc[:,1].values\n",
    "        edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.int64)\n",
    "\n",
    "        ## load the (x,y) formatted data\n",
    "        x = torch.load(self.raw_paths[1], map_location=torch.device('cpu'))\n",
    "        y = torch.load(self.raw_paths[2], map_location=torch.device('cpu'))\n",
    "        train_mask = torch.load(self.raw_paths[3], map_location=torch.device('cpu')) == 1 \n",
    "        test_mask = torch.load(self.raw_paths[4], map_location=torch.device('cpu')) == 1 \n",
    "\n",
    "        ## set the edge weights to be the duration (in hours)\n",
    "        edge_attributes = pd.read_csv(self.raw_paths[5])['duration'].values/3600\n",
    "        duration =  torch.FloatTensor(edge_attributes)\n",
    "        ## previous approaches used the degree:\n",
    "        #row, col = data.edge_index\n",
    "        #data.edge_attr = (1. / degree(col, data.num_nodes)[col]).double()\n",
    "        \n",
    "        ## build the data\n",
    "        data = Data(edge_index=edge_index, x=x, y=y, train_mask=train_mask, test_mask=test_mask)\n",
    "        data.edge_weight = duration\n",
    "        data.train_mask = train_mask\n",
    "        data.test_mask = test_mask\n",
    "        #data.train_mask = torch.cat((torch.ones(n_train, dtype=torch.bool), torch.zeros(n_val, dtype=torch.bool), torch.zeros(n_test, dtype=torch.bool)), dim=0)\n",
    "        #data.test_mask = torch.cat((torch.zeros(n_train, dtype=torch.bool), torch.zeros(n_val, dtype=torch.bool), torch.ones(n_test, dtype=torch.bool)), dim=0)\n",
    "\n",
    "        print(data.__dict__)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "{'x': tensor([[1., 0., 1.,  ..., 3., 2., 3.],\n",
      "        [0., 1., 1.,  ..., 3., 2., 3.],\n",
      "        [1., 0., 0.,  ..., 3., 2., 3.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 8., 1., 2.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.]]), 'edge_index': tensor([[      0,       0,       1,  ..., 1486224, 1378614, 1556530],\n",
      "        [      1,       2,       2,  ..., 1601329, 1601329, 1601329]]), 'edge_attr': None, 'y': tensor([[42.],\n",
      "        [43.],\n",
      "        [17.],\n",
      "        ...,\n",
      "        [ 0.],\n",
      "        [60.],\n",
      "        [79.]]), 'pos': None, 'norm': None, 'face': None, 'train_mask': tensor([True, True, True,  ..., True, True, True]), 'test_mask': tensor([False, False, False,  ..., False, False, False]), 'edge_weight': tensor([10.9161, 12.7494, 12.5828,  ...,  0.0497,  0.1667,  0.1667])}\n",
      "Done!\n",
      "{'x': tensor([[1., 0., 1.,  ..., 3., 2., 3.],\n",
      "        [0., 1., 1.,  ..., 3., 2., 3.],\n",
      "        [1., 0., 0.,  ..., 3., 2., 3.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 8., 1., 2.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.]]), 'edge_index': tensor([[      0,       0,       1,  ..., 1486224, 1378614, 1556530],\n",
      "        [      1,       2,       2,  ..., 1601329, 1601329, 1601329]]), 'edge_attr': None, 'y': tensor([[42.],\n",
      "        [43.],\n",
      "        [17.],\n",
      "        ...,\n",
      "        [ 0.],\n",
      "        [60.],\n",
      "        [79.]]), 'pos': None, 'norm': None, 'face': None, 'train_mask': tensor([True, True, True,  ..., True, True, True]), 'test_mask': tensor([False, False, False,  ..., False, False, False]), 'edge_weight': tensor([10.9161, 12.7494, 12.5828,  ...,  0.0497,  0.1667,  0.1667])}\n"
     ]
    }
   ],
   "source": [
    "## remove old processed files\n",
    "import shutil\n",
    "shutil.rmtree('../../data/NDSSL data/processed')\n",
    "\n",
    "## shuffle the masks\n",
    "dataset = NDSSLDataset('../../data/NDSSL data/')\n",
    "dataset.process()\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': '../../data/NDSSL data',\n",
       " 'transform': None,\n",
       " 'pre_transform': None,\n",
       " 'pre_filter': None,\n",
       " '__indices__': None,\n",
       " 'data': Data(edge_index=[2, 19681821], edge_weight=[19681821], test_mask=[1601330], train_mask=[1601330], x=[1601330, 25], y=[1601330, 1]),\n",
       " 'slices': {'x': tensor([      0, 1601330]),\n",
       "  'edge_index': tensor([       0, 19681821]),\n",
       "  'y': tensor([      0, 1601330]),\n",
       "  'train_mask': tensor([      0, 1601330]),\n",
       "  'test_mask': tensor([      0, 1601330]),\n",
       "  'edge_weight': tensor([       0, 19681821])}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I was initially having a hard time with GraphSAINT, and even for a simple example using the CORA dataset. My computer was crashing after pytorch ate up all the RAM. I believe this is realted to [this issue](https://github.com/rusty1s/pytorch_geometric/issues/1331). The problem was fixed when I set `num_workers=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute GraphSAINT normalization: : 17831578it [00:13, 1348345.44it/s]                            \n"
     ]
    }
   ],
   "source": [
    "loader = GraphSAINTRandomWalkSampler(data, batch_size=6000, walk_length=2,\n",
    "                                     num_steps=5, sample_coverage=10,\n",
    "                                     save_dir=dataset.processed_dir,\n",
    "                                     num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN\n",
    "\n",
    "documentation: https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=GCNConv#torch_geometric.nn.conv.GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels) \n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        #x = x.log_softmax(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 2029001\n"
     ]
    }
   ],
   "source": [
    "model = GCN(1000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        batch_size = out[data.train_mask].shape[0]\n",
    "        loss = nn.MSELoss()(out[data.train_mask], data.y[data.train_mask]) #F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_accuracy():\n",
    "    model.eval()\n",
    "    \n",
    "    total_examples_train = correct_train = 0.0\n",
    "    total_examples_test = correct_test = 0.0    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        \n",
    "        ## evaluate the train/test accuracies\n",
    "        train_mask = data.train_mask\n",
    "        test_mask = data.test_mask\n",
    "        total_examples_train += torch.sum(train_mask).item()\n",
    "        total_examples_test += torch.sum(test_mask).item()\n",
    "        correct = (torch.argmax(out, axis=1) == data.y)\n",
    "        \n",
    "        correct_train += torch.sum(correct * train_mask).cpu().item()        \n",
    "        correct_test += torch.sum(correct * test_mask).cpu().item()        \n",
    "    \n",
    "    overall_accuracy_train = correct_train/total_examples_train    \n",
    "    overall_accuracy_test = correct_test/total_examples_test    \n",
    "    \n",
    "    return overall_accuracy_train, overall_accuracy_test\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loss():\n",
    "    model.eval()\n",
    "    \n",
    "    total_examples_train = correct_train = 0.0\n",
    "    total_examples_test = correct_test = 0.0    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        \n",
    "        ## evaluate the train/test accuracies\n",
    "        train_mask = data.train_mask\n",
    "        test_mask = data.test_mask\n",
    "        total_examples_train += torch.sum(train_mask).item()\n",
    "        total_examples_test += torch.sum(test_mask).item()\n",
    "        \n",
    "        loss = nn.MSELoss()(out, data.y)\n",
    "        loss_train = (loss * train_mask)\n",
    "        loss_test = (loss * test_mask)\n",
    "    \n",
    "    return loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-0e93a1b71d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, '\u001b[0m\u001b[0;34mf'Test: {accs[1]:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pygeo/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ipow__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    accs = test_loss()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in loader:\n",
    "    data = data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.edge_weight)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels) \n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
    "        \n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "        self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        \n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "        x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x #x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 4056001\n"
     ]
    }
   ],
   "source": [
    "model = SAGE(1000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        batch_size = out[data.train_mask].shape[0]\n",
    "        loss = nn.MSELoss()(out[data.train_mask], data.y[data.train_mask]) #F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    model.set_aggr('mean')\n",
    "    \n",
    "    total_examples_train = correct_train = 0.0\n",
    "    total_examples_test = correct_test = 0.0    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        \n",
    "        ## evaluate the train/test accuracies\n",
    "        train_mask = data.train_mask\n",
    "        test_mask = data.test_mask\n",
    "        total_examples_train += torch.sum(train_mask).item()\n",
    "        total_examples_test += torch.sum(test_mask).item()\n",
    "        correct = (torch.argmax(out, axis=1) == data.y)\n",
    "        \n",
    "        correct_train += torch.sum(correct * train_mask).cpu().item()        \n",
    "        correct_test += torch.sum(correct * test_mask).cpu().item()        \n",
    "    \n",
    "    overall_accuracy_train = correct_train/total_examples_train    \n",
    "    overall_accuracy_test = correct_test/total_examples_test    \n",
    "    \n",
    "    return overall_accuracy_train, overall_accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1589.7677, Train: 235.9798, Test: 236.0897\n",
      "Epoch: 02, Loss: 1437.1159, Train: 248.9879, Test: 249.0423\n",
      "Epoch: 03, Loss: 1241.5081, Train: 232.1442, Test: 232.3878\n",
      "Epoch: 04, Loss: 1015.4713, Train: 244.0024, Test: 244.0045\n",
      "Epoch: 05, Loss: 835.5197, Train: 237.3943, Test: 237.4306\n",
      "Epoch: 06, Loss: 808.9143, Train: 238.1822, Test: 238.2721\n",
      "Epoch: 07, Loss: 811.4684, Train: 239.4648, Test: 239.2123\n",
      "Epoch: 08, Loss: 747.7600, Train: 231.2643, Test: 230.9504\n",
      "Epoch: 09, Loss: 712.8136, Train: 227.7885, Test: 227.8442\n",
      "Epoch: 10, Loss: 694.1336, Train: 229.9792, Test: 230.0539\n",
      "Epoch: 11, Loss: 661.3461, Train: 229.2236, Test: 229.1011\n",
      "Epoch: 12, Loss: 631.4715, Train: 249.7536, Test: 250.0268\n",
      "Epoch: 13, Loss: 608.5482, Train: 235.5926, Test: 235.6380\n",
      "Epoch: 14, Loss: 575.5143, Train: 245.1803, Test: 245.2773\n",
      "Epoch: 15, Loss: 549.3979, Train: 233.9833, Test: 234.0651\n",
      "Epoch: 16, Loss: 518.0651, Train: 230.4202, Test: 230.3286\n",
      "Epoch: 17, Loss: 492.4666, Train: 233.4544, Test: 233.1851\n",
      "Epoch: 18, Loss: 460.8100, Train: 231.8173, Test: 231.7626\n",
      "Epoch: 19, Loss: 423.2534, Train: 239.9188, Test: 240.3603\n",
      "Epoch: 20, Loss: 390.1924, Train: 238.3973, Test: 238.4280\n",
      "Epoch: 21, Loss: 359.2546, Train: 235.1794, Test: 235.3170\n",
      "Epoch: 22, Loss: 322.5359, Train: 225.5726, Test: 225.6958\n",
      "Epoch: 23, Loss: 290.4664, Train: 242.4121, Test: 242.3334\n",
      "Epoch: 24, Loss: 263.5913, Train: 231.8048, Test: 231.7788\n",
      "Epoch: 25, Loss: 242.4931, Train: 233.7942, Test: 233.8082\n",
      "Epoch: 26, Loss: 227.5382, Train: 233.5785, Test: 233.7392\n",
      "Epoch: 27, Loss: 215.7648, Train: 247.9966, Test: 248.0247\n",
      "Epoch: 28, Loss: 204.4028, Train: 227.4026, Test: 227.3603\n",
      "Epoch: 29, Loss: 196.1419, Train: 230.8036, Test: 230.7895\n",
      "Epoch: 30, Loss: 189.9048, Train: 232.0170, Test: 231.9368\n",
      "Epoch: 31, Loss: 184.4361, Train: 238.9950, Test: 239.0330\n",
      "Epoch: 32, Loss: 178.5418, Train: 218.9644, Test: 219.1307\n",
      "Epoch: 33, Loss: 174.0488, Train: 227.2094, Test: 227.1760\n",
      "Epoch: 34, Loss: 168.9114, Train: 243.1936, Test: 243.2078\n",
      "Epoch: 35, Loss: 162.9631, Train: 236.6090, Test: 236.5665\n",
      "Epoch: 36, Loss: 162.3282, Train: 242.4067, Test: 242.3877\n",
      "Epoch: 37, Loss: 158.8804, Train: 242.6017, Test: 242.5762\n",
      "Epoch: 38, Loss: 156.0905, Train: 227.4070, Test: 227.3538\n",
      "Epoch: 39, Loss: 153.0522, Train: 246.3984, Test: 246.3869\n",
      "Epoch: 40, Loss: 151.2156, Train: 233.7974, Test: 233.8147\n",
      "Epoch: 41, Loss: 149.3987, Train: 230.7895, Test: 230.8391\n",
      "Epoch: 42, Loss: 148.9191, Train: 243.4105, Test: 243.3998\n",
      "Epoch: 43, Loss: 147.2197, Train: 234.5784, Test: 234.6966\n",
      "Epoch: 44, Loss: 145.1413, Train: 227.4360, Test: 227.2941\n",
      "Epoch: 45, Loss: 145.0436, Train: 243.2111, Test: 243.1769\n",
      "Epoch: 46, Loss: 143.7701, Train: 236.8018, Test: 236.7652\n",
      "Epoch: 47, Loss: 142.8565, Train: 234.8019, Test: 234.8279\n",
      "Epoch: 48, Loss: 139.8157, Train: 230.1902, Test: 230.2431\n",
      "Epoch: 49, Loss: 140.5797, Train: 224.5586, Test: 224.7436\n",
      "Epoch: 50, Loss: 138.8433, Train: 227.0046, Test: 226.9702\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch geometric)",
   "language": "python",
   "name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
