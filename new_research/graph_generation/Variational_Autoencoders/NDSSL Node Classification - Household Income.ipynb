{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDSSL Node Classification - Household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.data import GraphSAINTRandomWalkSampler\n",
    "from torch_geometric.utils import get_laplacian, degree\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.style as style \n",
    "style.use('seaborn-paper')\n",
    "\n",
    "fontsize = 12\n",
    "plt.rcParams.update({\n",
    "    'font.size': fontsize, \n",
    "    'axes.labelsize': fontsize, \n",
    "    'legend.fontsize': fontsize,\n",
    "    'xtick.labelsize': fontsize,\n",
    "    'ytick.labelsize': fontsize,\n",
    "    'axes.titlesize': fontsize\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#from imports import *\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Convert the node attribute data into an (x,y) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = pd.read_csv('../../data/NDSSL data/raw/node_attributes.csv')\n",
    "\n",
    "## one-hot encode gender\n",
    "gender_index = torch.LongTensor(node_attributes['gender'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "gender_onehot = torch.LongTensor(len(node_attributes), 2)\n",
    "gender_onehot.zero_()\n",
    "gender_onehot = gender_onehot.scatter_(1, gender_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode worker\n",
    "worker_index = torch.LongTensor(node_attributes['worker'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "worker_onehot = torch.LongTensor(len(node_attributes), 2)\n",
    "worker_onehot.zero_()\n",
    "worker_onehot = worker_onehot.scatter_(1, worker_index, 1).type(torch.float32);\n",
    "\n",
    "## map the 117 distinct zipcodes to the integers 0, ..., 116\n",
    "zipcode_original = node_attributes['zipcode'].values\n",
    "zipcode_dict = {i: j for j, i in enumerate(set(zipcode_original))} \n",
    "zipcode_index = torch.LongTensor(np.asarray([zipcode_dict[i] for i in zipcode_original])).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "\n",
    "## one-hot encode zipcode\n",
    "zipcode_onehot = torch.LongTensor(len(node_attributes), len(zipcode_dict))\n",
    "zipcode_onehot.zero_()\n",
    "zipcode_onehot = zipcode_onehot.scatter_(1, zipcode_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode household income\n",
    "household_income_index = torch.LongTensor(node_attributes['household_income'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "household_income_onehot = torch.LongTensor(len(node_attributes), 14)\n",
    "household_income_onehot.zero_()\n",
    "household_income_onehot = household_income_onehot.scatter_(1, household_income_index, 1).type(torch.float32);\n",
    "\n",
    "## one-hot encode relationship\n",
    "relationship_index = torch.LongTensor(node_attributes['relationship'].values - 1).type(torch.int64).reshape((len(node_attributes), 1))\n",
    "relationship_onehot = torch.LongTensor(len(node_attributes), 4)\n",
    "relationship_onehot.zero_()\n",
    "relationship_onehot = relationship_onehot.scatter_(1, relationship_index, 1).type(torch.float32);\n",
    "\n",
    "age = torch.FloatTensor(node_attributes['age'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_size = torch.FloatTensor(node_attributes['household_size'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_workers = torch.FloatTensor(node_attributes['household_workers'].values).reshape(len(node_attributes), 1).type(torch.float32)\n",
    "household_vehicles = torch.FloatTensor(node_attributes['household_vehicles'].values).reshape(len(node_attributes), 1).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1601330, 129]) torch.float32\n",
      "torch.Size([1601330]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.cat((gender_onehot, worker_onehot, age, relationship_onehot, zipcode_onehot, household_size, household_workers, household_vehicles), dim=1)\n",
    "y = torch.LongTensor(node_attributes['household_income'].values - 1).type(torch.int64)#.reshape((len(node_attributes)), 1)\n",
    "\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the household income is not evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.02553377504949011,\n",
       " 1: 0.04408210674876509,\n",
       " 2: 0.05800241049627497,\n",
       " 3: 0.06630238614151986,\n",
       " 4: 0.07642022568739736,\n",
       " 5: 0.08593856356903325,\n",
       " 6: 0.09993630294817434,\n",
       " 7: 0.08903536435338125,\n",
       " 8: 0.07680865280735388,\n",
       " 9: 0.0698057239919317,\n",
       " 10: 0.059364403339723856,\n",
       " 11: 0.045411626585400884,\n",
       " 12: 0.20297065564249717,\n",
       " 13: 0.0003878026390562845}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_membership = {i:torch.sum(y == i).item()/len(node_attributes) for i in range(14)}\n",
    "class_membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the distribution of household income *by individual*, whereas earlier in the data analysis workflow we plotted the distribution of household income *by household*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x316.8 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y.numpy(), alpha=0.8)\n",
    "plt.title('Household Income Distribution')\n",
    "plt.xlabel('household income (class)')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will train machine learning classifiers. Before we do so, let's see how good we can do with just a few simple (non-learned) guessing strategies. These will serve as good baselines for any model we train. \n",
    "\n",
    "There are at least 3 simple ways to guess\n",
    "1. uniform random guessing (each class is equally likely)\n",
    "2. weighted random guessing (pick each class according to it's frequency)\n",
    "3. always pick the most popular class\n",
    "\n",
    "The theoretical result for each of these strategies can be easily worked out. If we pick the classes randomly with weights $x_i$, and the true class weights are $w_i$, then the expected accuracy is:\n",
    "$$ \\mathbb{E} \\text{acc} = \\sum_{i=1}^C w_i x_i .$$\n",
    "For strategy 1., $x_i = 1/C$, the expected accuracy is just $1/C$, as the weights sum to 1. For strategy 2., $x_i = w_i$. Strategy 3 is straightforwardly going to give just $\\max(w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform random guessing: 0.07\n",
      "theoretical prediction: 0.07\n",
      "\n",
      "weighted random guessing: 0.10\n",
      "theoretical prediction: 0.10\n",
      "\n",
      "always pick most popular class: 0.20\n",
      "theoretical prediction: 0.20\n"
     ]
    }
   ],
   "source": [
    "print('uniform random guessing: %.2f' %np.mean(y.numpy() == np.random.choice(14, len(y))))\n",
    "print('theoretical prediction: %.2f' % (1.0/14) )\n",
    "\n",
    "class_probs = np.asarray(list(class_membership.values()))\n",
    "print('\\nweighted random guessing: %.2f' %np.mean(y.numpy() == np.random.choice(14, len(y), p=class_probs)))\n",
    "print('theoretical prediction: %.2f' % np.sum(class_probs**2))\n",
    "\n",
    "print('\\nalways pick most popular class: %.2f' %np.mean(y.numpy() == np.ones(1)*np.argmax(class_probs)))\n",
    "print('theoretical prediction: %.2f' % class_probs[np.argmax(class_probs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct households: 632626\n",
      "number of training households: 506100\n",
      "number of testing households: 126526\n",
      "size of training set: 1280463\n",
      "size of testing set: 320867\n",
      "percent train: 0.80\n"
     ]
    }
   ],
   "source": [
    "## create a train/test split by household\n",
    "household_ids = list(set(list(node_attributes['household_id'])))\n",
    "np.random.shuffle(household_ids)\n",
    "\n",
    "ntrain_households = int(0.8 * len(household_ids))\n",
    "households_train = household_ids[:ntrain_households]\n",
    "households_test = household_ids[ntrain_households:]\n",
    "print('number of distinct households: %i' %len(household_ids))\n",
    "print('number of training households: %i' %len(households_train))\n",
    "print('number of testing households: %i' %len(households_test))\n",
    "\n",
    "train_mask = torch.IntTensor(node_attributes['household_id'].isin(households_train)).type(torch.int64)\n",
    "test_mask = torch.IntTensor(node_attributes['household_id'].isin(households_test)).type(torch.int64)\n",
    "train_idx = np.arange(len(node_attributes))[train_mask == 1]\n",
    "test_idx = np.arange(len(node_attributes))[test_mask == 1]\n",
    "\n",
    "print('size of training set: %i' %torch.sum(train_mask).item())\n",
    "print('size of testing set: %i' %torch.sum(test_mask).item())\n",
    "print('percent train: %.2f' %(torch.sum(train_mask).item()/len(train_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, '../../data/NDSSL data/raw/x.pt')\n",
    "torch.save(y, '../../data/NDSSL data/raw/y.pt')\n",
    "torch.save(train_mask, '../../data/NDSSL data/raw/train_mask.pt')\n",
    "torch.save(test_mask, '../../data/NDSSL data/raw/test_mask.pt')\n",
    "torch.save(train_idx, '../../data/NDSSL data/raw/train_idx.pt')\n",
    "torch.save(test_idx, '../../data/NDSSL data/raw/test_idx.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification: MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = torch.utils.data.TensorDataset(x, y)\n",
    "train_set = torch.utils.data.Subset(full_set, train_idx)\n",
    "test_set = torch.utils.data.Subset(full_set, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(mlp, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)    \n",
    "        x = x.log_softmax(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.Tensor(class_membership[12]/np.asarray(list(class_membership.values()))).float().to(device)\n",
    "weight = torch.ones(len(class_membership)).float().to(device)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        loss = F.nll_loss(out, y_batch, weight=weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_batch.shape[0]\n",
    "        total_examples += x_batch.shape[0]\n",
    "    \n",
    "    return total_loss/total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    ## train accuracy\n",
    "    total_examples = correct = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        total_examples += out.shape[0]\n",
    "        correct += torch.sum(torch.argmax(out, axis=1) == y_batch).cpu().item()\n",
    "    train_accuracy = correct/total_examples\n",
    "    \n",
    "    ## test accuracy\n",
    "    total_examples = correct = 0.0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        total_examples += x_batch.shape[0]\n",
    "        correct += torch.sum(torch.argmax(out, axis=1) == y_batch).cpu().item()\n",
    "    test_accuracy = correct/total_examples\n",
    "    \n",
    "    return train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: why is this so slow, even with a GPU?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters 1145014\n",
      "Epoch: 01, Loss: 2.5423, Train: 0.2036, Test: 0.2003\n",
      "Epoch: 02, Loss: 2.4490, Train: 0.2208, Test: 0.2175\n",
      "Epoch: 03, Loss: 2.4041, Train: 0.2218, Test: 0.2189\n",
      "Epoch: 04, Loss: 2.3745, Train: 0.2275, Test: 0.2245\n",
      "Epoch: 05, Loss: 2.3526, Train: 0.2284, Test: 0.2254\n",
      "Epoch: 06, Loss: 2.3345, Train: 0.2288, Test: 0.2259\n",
      "Epoch: 07, Loss: 2.3191, Train: 0.2292, Test: 0.2262\n",
      "Epoch: 08, Loss: 2.3055, Train: 0.2296, Test: 0.2266\n",
      "Epoch: 09, Loss: 2.2941, Train: 0.2303, Test: 0.2273\n",
      "Epoch: 10, Loss: 2.2839, Train: 0.2322, Test: 0.2290\n",
      "Epoch: 11, Loss: 2.2750, Train: 0.2340, Test: 0.2306\n",
      "Epoch: 12, Loss: 2.2671, Train: 0.2353, Test: 0.2316\n",
      "Epoch: 13, Loss: 2.2606, Train: 0.2362, Test: 0.2323\n",
      "Epoch: 14, Loss: 2.2542, Train: 0.2385, Test: 0.2348\n",
      "Epoch: 15, Loss: 2.2489, Train: 0.2397, Test: 0.2360\n",
      "Epoch: 16, Loss: 2.2436, Train: 0.2411, Test: 0.2375\n",
      "Epoch: 17, Loss: 2.2390, Train: 0.2428, Test: 0.2392\n",
      "Epoch: 18, Loss: 2.2346, Train: 0.2448, Test: 0.2413\n",
      "Epoch: 19, Loss: 2.2309, Train: 0.2467, Test: 0.2434\n",
      "Epoch: 20, Loss: 2.2270, Train: 0.2489, Test: 0.2456\n",
      "Epoch: 21, Loss: 2.2234, Train: 0.2510, Test: 0.2477\n",
      "Epoch: 22, Loss: 2.2199, Train: 0.2523, Test: 0.2488\n",
      "Epoch: 23, Loss: 2.2167, Train: 0.2533, Test: 0.2498\n",
      "Epoch: 24, Loss: 2.2137, Train: 0.2543, Test: 0.2508\n",
      "Epoch: 25, Loss: 2.2110, Train: 0.2553, Test: 0.2516\n",
      "Epoch: 26, Loss: 2.2081, Train: 0.2558, Test: 0.2524\n",
      "Epoch: 27, Loss: 2.2056, Train: 0.2564, Test: 0.2530\n",
      "Epoch: 28, Loss: 2.2032, Train: 0.2568, Test: 0.2535\n",
      "Epoch: 29, Loss: 2.2003, Train: 0.2570, Test: 0.2534\n",
      "Epoch: 30, Loss: 2.1982, Train: 0.2575, Test: 0.2541\n",
      "Epoch: 31, Loss: 2.1959, Train: 0.2581, Test: 0.2545\n",
      "Epoch: 32, Loss: 2.1934, Train: 0.2584, Test: 0.2549\n",
      "Epoch: 33, Loss: 2.1912, Train: 0.2588, Test: 0.2552\n",
      "Epoch: 34, Loss: 2.1891, Train: 0.2588, Test: 0.2554\n",
      "Epoch: 35, Loss: 2.1875, Train: 0.2589, Test: 0.2554\n",
      "Epoch: 36, Loss: 2.1854, Train: 0.2593, Test: 0.2557\n",
      "Epoch: 37, Loss: 2.1836, Train: 0.2600, Test: 0.2567\n",
      "Epoch: 38, Loss: 2.1817, Train: 0.2600, Test: 0.2568\n",
      "Epoch: 39, Loss: 2.1797, Train: 0.2605, Test: 0.2572\n",
      "Epoch: 40, Loss: 2.1781, Train: 0.2610, Test: 0.2578\n",
      "Epoch: 41, Loss: 2.1766, Train: 0.2616, Test: 0.2582\n",
      "Epoch: 42, Loss: 2.1749, Train: 0.2615, Test: 0.2583\n",
      "Epoch: 43, Loss: 2.1733, Train: 0.2622, Test: 0.2590\n",
      "Epoch: 44, Loss: 2.1719, Train: 0.2620, Test: 0.2586\n",
      "Epoch: 45, Loss: 2.1706, Train: 0.2642, Test: 0.2607\n",
      "Epoch: 46, Loss: 2.1689, Train: 0.2626, Test: 0.2594\n",
      "Epoch: 47, Loss: 2.1672, Train: 0.2630, Test: 0.2598\n",
      "Epoch: 48, Loss: 2.1662, Train: 0.2636, Test: 0.2603\n",
      "Epoch: 49, Loss: 2.1647, Train: 0.2637, Test: 0.2605\n",
      "Epoch: 50, Loss: 2.1634, Train: 0.2649, Test: 0.2615\n"
     ]
    }
   ],
   "source": [
    "model = mlp(x.shape[1], 1000, 14)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "print('number of trainable parameters %i' %sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 1000\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification: Message Passing NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDSSLDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(NDSSLDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['edge_list.csv', 'x.pt', 'y.pt', 'train_mask.pt', 'test_mask.pt', 'edge_attributes.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['NDSSL_graph_full.pt']\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        \n",
    "        ## load the edge list\n",
    "        edge_list = pd.read_csv(self.raw_paths[0], dtype=int) - 2000000 #the node id's start at 2000000, shift these to start at 0         \n",
    "        \n",
    "        ## format the edge list\n",
    "        target_nodes = edge_list.iloc[:,0].values\n",
    "        source_nodes = edge_list.iloc[:,1].values\n",
    "        edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.int64)\n",
    "\n",
    "        ## load the (x,y) formatted data\n",
    "        x = torch.load(self.raw_paths[1], map_location=torch.device('cpu'))\n",
    "        y = torch.load(self.raw_paths[2], map_location=torch.device('cpu'))\n",
    "        train_mask = torch.load(self.raw_paths[3], map_location=torch.device('cpu')) == 1 \n",
    "        test_mask = torch.load(self.raw_paths[4], map_location=torch.device('cpu')) == 1 \n",
    "\n",
    "        ## set the edge weights to be the duration (in hours)\n",
    "        edge_attributes = pd.read_csv(self.raw_paths[5])['duration'].values/3600\n",
    "        duration =  torch.FloatTensor(edge_attributes)\n",
    "        ## previous approaches used the degree:\n",
    "        #row, col = data.edge_index\n",
    "        #data.edge_attr = (1. / degree(col, data.num_nodes)[col]).double()\n",
    "        \n",
    "        ## build the data\n",
    "        data = Data(edge_index=edge_index, x=x, y=y, train_mask=train_mask, test_mask=test_mask)\n",
    "        data.edge_weight = duration\n",
    "        data.train_mask = train_mask\n",
    "        data.test_mask = test_mask\n",
    "        #data.train_mask = torch.cat((torch.ones(n_train, dtype=torch.bool), torch.zeros(n_val, dtype=torch.bool), torch.zeros(n_test, dtype=torch.bool)), dim=0)\n",
    "        #data.test_mask = torch.cat((torch.zeros(n_train, dtype=torch.bool), torch.zeros(n_val, dtype=torch.bool), torch.ones(n_test, dtype=torch.bool)), dim=0)\n",
    "\n",
    "        print(data.__dict__)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "{'x': tensor([[1., 0., 1.,  ..., 3., 2., 3.],\n",
      "        [0., 1., 1.,  ..., 3., 2., 3.],\n",
      "        [1., 0., 0.,  ..., 3., 2., 3.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 8., 1., 2.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.]]), 'edge_index': tensor([[      0,       0,       1,  ..., 1486224, 1378614, 1556530],\n",
      "        [      1,       2,       2,  ..., 1601329, 1601329, 1601329]]), 'edge_attr': None, 'y': tensor([12, 12, 12,  ...,  5,  4,  2]), 'pos': None, 'norm': None, 'face': None, 'train_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'test_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'edge_weight': tensor([10.9161, 12.7494, 12.5828,  ...,  0.0497,  0.1667,  0.1667])}\n",
      "Done!\n",
      "{'x': tensor([[1., 0., 1.,  ..., 3., 2., 3.],\n",
      "        [0., 1., 1.,  ..., 3., 2., 3.],\n",
      "        [1., 0., 0.,  ..., 3., 2., 3.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 8., 1., 2.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.]]), 'edge_index': tensor([[      0,       0,       1,  ..., 1486224, 1378614, 1556530],\n",
      "        [      1,       2,       2,  ..., 1601329, 1601329, 1601329]]), 'edge_attr': None, 'y': tensor([12, 12, 12,  ...,  5,  4,  2]), 'pos': None, 'norm': None, 'face': None, 'train_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'test_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'edge_weight': tensor([10.9161, 12.7494, 12.5828,  ...,  0.0497,  0.1667,  0.1667])}\n"
     ]
    }
   ],
   "source": [
    "## remove old processed files\n",
    "import shutil\n",
    "shutil.rmtree('../../data/NDSSL data/processed')\n",
    "\n",
    "## shuffle the masks\n",
    "dataset = NDSSLDataset('../../data/NDSSL data/')\n",
    "dataset.process()\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': '../../data/NDSSL data',\n",
       " 'transform': None,\n",
       " 'pre_transform': None,\n",
       " 'pre_filter': None,\n",
       " '__indices__': None,\n",
       " 'data': Data(edge_index=[2, 19681821], edge_weight=[19681821], test_mask=[1601330], train_mask=[1601330], x=[1601330, 129], y=[1601330]),\n",
       " 'slices': {'x': tensor([      0, 1601330]),\n",
       "  'edge_index': tensor([       0, 19681821]),\n",
       "  'y': tensor([      0, 1601330]),\n",
       "  'train_mask': tensor([      0, 1601330]),\n",
       "  'test_mask': tensor([      0, 1601330]),\n",
       "  'edge_weight': tensor([       0, 19681821])}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I was initially having a hard time with GraphSAINT, and even for a simple example using the CORA dataset. My computer was crashing after pytorch ate up all the RAM. I believe this is realted to [this issue](https://github.com/rusty1s/pytorch_geometric/issues/1331). The problem was fixed when I set `num_workers=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "def train_test_split_big(data, val_ratio=0.05, test_ratio=0.1):\n",
    "    row, col = data.edge_index\n",
    "    # data.edge_index = None\n",
    "\n",
    "    # Return upper triangular portion.\n",
    "    mask = row < col\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    n_v = int(math.floor(val_ratio * row.size(0)))\n",
    "    n_t = int(math.floor(test_ratio * row.size(0)))\n",
    "\n",
    "    # Positive edges.\n",
    "    perm = torch.randperm(row.size(0))\n",
    "    row, col = row[perm], col[perm]\n",
    "\n",
    "    r, c = row[:n_v], col[:n_v]\n",
    "    data.val_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    r, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]\n",
    "    data.test_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "\n",
    "    r, c = row[n_v + n_t:], col[n_v + n_t:]\n",
    "    data.train_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.train_pos_edge_index = to_undirected(data.train_pos_edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute GraphSAINT normalization: : 17296808it [00:15, 1087543.59it/s]                            \n",
      "Compute GraphSAINT normalization: : 17680853it [00:09, 1828083.54it/s]                            \n"
     ]
    }
   ],
   "source": [
    "data = train_test_split_big(data, test_ratio=0.1)\n",
    "train_data = Data(x=data.x, edge_index=data.train_pos_edge_index, y=data.y)\n",
    "test_data = Data(x=data.x, edge_index=data.test_pos_edge_index, y=data.y)\n",
    "\n",
    "train_loader = GraphSAINTRandomWalkSampler(train_data, batch_size=6000, walk_length=2,\n",
    "                                     num_steps=5, sample_coverage=10,\n",
    "                                     save_dir=None)#dataset.processed_dir)\n",
    "\n",
    "test_loader = GraphSAINTRandomWalkSampler(test_data, batch_size=6000, walk_length=2,\n",
    "                                     num_steps=5, sample_coverage=10,\n",
    "                                     save_dir=None)#dataset.processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN\n",
    "\n",
    "documentation: https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=GCNConv#torch_geometric.nn.conv.GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels) \n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(1000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        \n",
    "        #batch_size = out[data.train_mask].shape[0]\n",
    "        #loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        batch_size = out.shape[0]\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    total_examples_train, correct_train = 0, 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        #print(data.x.shape, data.y.shape, data.edge_index.shape)\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        total_examples_train += out.shape[0]\n",
    "        #print(out.shape, correct_train)\n",
    "        correct_train += torch.sum((torch.argmax(out, axis=1) == data.y)).item()\n",
    "    accuracy_train = correct_train/total_examples_train\n",
    "    \n",
    "    total_examples_test, correct_test = 0, 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        total_examples_test += out.shape[0]\n",
    "        correct_test += torch.sum((torch.argmax(out, axis=1) == data.y)).item()\n",
    "    accuracy_test = correct_test/total_examples_test   \n",
    "    \n",
    "    return accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why is the test accuracy better than the training accuracy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 2.8885, Train: 0.0577, Test: 0.0642\n",
      "Epoch: 02, Loss: 2.5150, Train: 0.2026, Test: 0.2029\n",
      "Epoch: 03, Loss: 2.4836, Train: 0.2032, Test: 0.2036\n",
      "Epoch: 04, Loss: 2.4639, Train: 0.2052, Test: 0.2024\n",
      "Epoch: 05, Loss: 2.4486, Train: 0.2030, Test: 0.2027\n",
      "Epoch: 06, Loss: 2.4298, Train: 0.2069, Test: 0.2112\n",
      "Epoch: 07, Loss: 2.4185, Train: 0.2053, Test: 0.2147\n",
      "Epoch: 08, Loss: 2.3991, Train: 0.2065, Test: 0.2158\n",
      "Epoch: 09, Loss: 2.3887, Train: 0.2126, Test: 0.2210\n",
      "Epoch: 10, Loss: 2.3847, Train: 0.2117, Test: 0.2255\n",
      "Epoch: 11, Loss: 2.3751, Train: 0.2125, Test: 0.2272\n",
      "Epoch: 12, Loss: 2.3655, Train: 0.2139, Test: 0.2297\n",
      "Epoch: 13, Loss: 2.3646, Train: 0.2154, Test: 0.2272\n",
      "Epoch: 14, Loss: 2.3626, Train: 0.2127, Test: 0.2256\n",
      "Epoch: 15, Loss: 2.3611, Train: 0.2142, Test: 0.2315\n",
      "Epoch: 16, Loss: 2.3571, Train: 0.2148, Test: 0.2280\n",
      "Epoch: 17, Loss: 2.3520, Train: 0.2176, Test: 0.2309\n",
      "Epoch: 18, Loss: 2.3519, Train: 0.2155, Test: 0.2299\n",
      "Epoch: 19, Loss: 2.3467, Train: 0.2184, Test: 0.2315\n",
      "Epoch: 20, Loss: 2.3530, Train: 0.2190, Test: 0.2363\n",
      "Epoch: 21, Loss: 2.3444, Train: 0.2170, Test: 0.2342\n",
      "Epoch: 22, Loss: 2.3477, Train: 0.2170, Test: 0.2349\n",
      "Epoch: 23, Loss: 2.3515, Train: 0.2181, Test: 0.2330\n",
      "Epoch: 24, Loss: 2.3478, Train: 0.2202, Test: 0.2256\n",
      "Epoch: 25, Loss: 2.3472, Train: 0.2183, Test: 0.2290\n",
      "Epoch: 26, Loss: 2.3462, Train: 0.2206, Test: 0.2300\n",
      "Epoch: 27, Loss: 2.3457, Train: 0.2186, Test: 0.2286\n",
      "Epoch: 28, Loss: 2.3432, Train: 0.2186, Test: 0.2341\n",
      "Epoch: 29, Loss: 2.3494, Train: 0.2211, Test: 0.2323\n",
      "Epoch: 30, Loss: 2.3408, Train: 0.2185, Test: 0.2305\n",
      "Epoch: 31, Loss: 2.3380, Train: 0.2216, Test: 0.2263\n",
      "Epoch: 32, Loss: 2.3368, Train: 0.2201, Test: 0.2343\n",
      "Epoch: 33, Loss: 2.3402, Train: 0.2169, Test: 0.2264\n",
      "Epoch: 34, Loss: 2.3403, Train: 0.2198, Test: 0.2324\n",
      "Epoch: 35, Loss: 2.3365, Train: 0.2185, Test: 0.2291\n",
      "Epoch: 36, Loss: 2.3345, Train: 0.2235, Test: 0.2346\n",
      "Epoch: 37, Loss: 2.3317, Train: 0.2243, Test: 0.2312\n",
      "Epoch: 38, Loss: 2.3372, Train: 0.2212, Test: 0.2289\n",
      "Epoch: 39, Loss: 2.3390, Train: 0.2197, Test: 0.2323\n",
      "Epoch: 40, Loss: 2.3341, Train: 0.2232, Test: 0.2350\n",
      "Epoch: 41, Loss: 2.3368, Train: 0.2193, Test: 0.2311\n",
      "Epoch: 42, Loss: 2.3320, Train: 0.2189, Test: 0.2385\n",
      "Epoch: 43, Loss: 2.3349, Train: 0.2178, Test: 0.2238\n",
      "Epoch: 44, Loss: 2.3360, Train: 0.2209, Test: 0.2357\n",
      "Epoch: 45, Loss: 2.3302, Train: 0.2212, Test: 0.2340\n",
      "Epoch: 46, Loss: 2.3295, Train: 0.2204, Test: 0.2373\n",
      "Epoch: 47, Loss: 2.3310, Train: 0.2225, Test: 0.2308\n",
      "Epoch: 48, Loss: 2.3341, Train: 0.2211, Test: 0.2322\n",
      "Epoch: 49, Loss: 2.3265, Train: 0.2182, Test: 0.2364\n",
      "Epoch: 50, Loss: 2.3298, Train: 0.2210, Test: 0.2313\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels) \n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
    "        \n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "        self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "        x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "        x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        return x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(1000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        \n",
    "        #batch_size = out[data.train_mask].shape[0]\n",
    "        #loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        batch_size = out.shape[0]\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    total_examples_train, correct_train = 0, 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        #print(data.x.shape, data.y.shape, data.edge_index.shape)\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        total_examples_train += out.shape[0]\n",
    "        #print(out.shape, correct_train)\n",
    "        correct_train += torch.sum((torch.argmax(out, axis=1) == data.y)).item()\n",
    "    accuracy_train = correct_train/total_examples_train\n",
    "    \n",
    "    total_examples_test, correct_test = 0, 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)#, data.edge_weight)\n",
    "        total_examples_test += out.shape[0]\n",
    "        correct_test += torch.sum((torch.argmax(out, axis=1) == data.y)).item()\n",
    "    accuracy_test = correct_test/total_examples_test   \n",
    "    \n",
    "    return accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 6.0748, Train: 0.0871, Test: 0.0829\n",
      "Epoch: 02, Loss: 2.9607, Train: 0.1143, Test: 0.0863\n",
      "Epoch: 03, Loss: 2.6205, Train: 0.2021, Test: 0.2042\n",
      "Epoch: 04, Loss: 2.5374, Train: 0.2079, Test: 0.2169\n",
      "Epoch: 05, Loss: 2.4565, Train: 0.2080, Test: 0.2008\n",
      "Epoch: 06, Loss: 2.4290, Train: 0.2061, Test: 0.2122\n",
      "Epoch: 07, Loss: 2.3958, Train: 0.2089, Test: 0.2147\n",
      "Epoch: 08, Loss: 2.3640, Train: 0.2170, Test: 0.2231\n",
      "Epoch: 09, Loss: 2.3374, Train: 0.2257, Test: 0.2226\n",
      "Epoch: 10, Loss: 2.3113, Train: 0.2316, Test: 0.2274\n",
      "Epoch: 11, Loss: 2.2974, Train: 0.2380, Test: 0.2346\n",
      "Epoch: 12, Loss: 2.2663, Train: 0.2469, Test: 0.2370\n",
      "Epoch: 13, Loss: 2.2493, Train: 0.2506, Test: 0.2369\n",
      "Epoch: 14, Loss: 2.2284, Train: 0.2525, Test: 0.2378\n",
      "Epoch: 15, Loss: 2.2104, Train: 0.2596, Test: 0.2403\n",
      "Epoch: 16, Loss: 2.1987, Train: 0.2619, Test: 0.2478\n",
      "Epoch: 17, Loss: 2.1888, Train: 0.2650, Test: 0.2483\n",
      "Epoch: 18, Loss: 2.1839, Train: 0.2687, Test: 0.2546\n",
      "Epoch: 19, Loss: 2.1729, Train: 0.2702, Test: 0.2515\n",
      "Epoch: 20, Loss: 2.1651, Train: 0.2716, Test: 0.2523\n",
      "Epoch: 21, Loss: 2.1618, Train: 0.2732, Test: 0.2591\n",
      "Epoch: 22, Loss: 2.1515, Train: 0.2752, Test: 0.2610\n",
      "Epoch: 23, Loss: 2.1474, Train: 0.2760, Test: 0.2610\n",
      "Epoch: 24, Loss: 2.1506, Train: 0.2739, Test: 0.2576\n",
      "Epoch: 25, Loss: 2.1409, Train: 0.2792, Test: 0.2600\n",
      "Epoch: 26, Loss: 2.1332, Train: 0.2742, Test: 0.2628\n",
      "Epoch: 27, Loss: 2.1305, Train: 0.2784, Test: 0.2648\n",
      "Epoch: 28, Loss: 2.1243, Train: 0.2806, Test: 0.2636\n",
      "Epoch: 29, Loss: 2.1255, Train: 0.2799, Test: 0.2650\n",
      "Epoch: 30, Loss: 2.1257, Train: 0.2799, Test: 0.2657\n",
      "Epoch: 31, Loss: 2.1245, Train: 0.2825, Test: 0.2658\n",
      "Epoch: 32, Loss: 2.1170, Train: 0.2775, Test: 0.2642\n",
      "Epoch: 33, Loss: 2.1151, Train: 0.2837, Test: 0.2657\n",
      "Epoch: 34, Loss: 2.1187, Train: 0.2817, Test: 0.2696\n",
      "Epoch: 35, Loss: 2.1188, Train: 0.2817, Test: 0.2732\n",
      "Epoch: 36, Loss: 2.1128, Train: 0.2833, Test: 0.2653\n",
      "Epoch: 37, Loss: 2.1069, Train: 0.2839, Test: 0.2667\n",
      "Epoch: 38, Loss: 2.1110, Train: 0.2801, Test: 0.2700\n",
      "Epoch: 39, Loss: 2.1040, Train: 0.2822, Test: 0.2708\n",
      "Epoch: 40, Loss: 2.1034, Train: 0.2848, Test: 0.2701\n",
      "Epoch: 41, Loss: 2.0999, Train: 0.2837, Test: 0.2709\n",
      "Epoch: 42, Loss: 2.0979, Train: 0.2860, Test: 0.2713\n",
      "Epoch: 43, Loss: 2.0970, Train: 0.2812, Test: 0.2721\n",
      "Epoch: 44, Loss: 2.0981, Train: 0.2853, Test: 0.2726\n",
      "Epoch: 45, Loss: 2.0993, Train: 0.2867, Test: 0.2723\n",
      "Epoch: 46, Loss: 2.0937, Train: 0.2876, Test: 0.2740\n",
      "Epoch: 47, Loss: 2.0917, Train: 0.2838, Test: 0.2752\n",
      "Epoch: 48, Loss: 2.0919, Train: 0.2881, Test: 0.2751\n",
      "Epoch: 49, Loss: 2.0864, Train: 0.2838, Test: 0.2708\n",
      "Epoch: 50, Loss: 2.0834, Train: 0.2815, Test: 0.2729\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, 'f'Test: {accs[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch geometric)",
   "language": "python",
   "name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
